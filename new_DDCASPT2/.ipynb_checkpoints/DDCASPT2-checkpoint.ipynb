{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install --upgrade  xeus-python notebook\n",
    "#######################################################\n",
    "#Import packages\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['MOLCAS']='/home/grierjones/Test/build'\n",
    "os.environ['MOLCAS_WORKDIR']='/tmp'\n",
    "import re\n",
    "from math import sin, cos, pi\n",
    "from glob import glob\n",
    "import subprocess\n",
    "import pickle\n",
    "from subprocess import call, check_output\n",
    "import pandas as pd\n",
    "# import psi4\n",
    "from joblib import Parallel,effective_n_jobs,delayed\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from plumbum.cmd import grep, awk\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "import sklearn\n",
    "from shutil import copy\n",
    "import csv\n",
    "import h5py as h5\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Obital labels\n",
    "## Inactive i,j\n",
    "## Active t,u,v\n",
    "## Virtual a,b\n",
    "\n",
    "## Type 1: IA->AA\n",
    "## Type 2: II->AA (P)\n",
    "## Type 3: II->AA (M)\n",
    "## Type 4: AA->VA\n",
    "## Type 5: IA->VA/AV\n",
    "## Type 6: II->AV (P)\n",
    "## Type 7: II->AV (M)\n",
    "## Type 8: AA->VV (P)\n",
    "## Type 9: AA->VV (M)\n",
    "## Type 10: IA->VV (P)\n",
    "## Type 11: IA->VV (M)\n",
    "## Type 12: II->VV (P)\n",
    "## Type 13: II->VV (M)\n",
    "\n",
    "## A: IA->AA\n",
    "## B: II->AA\n",
    "## C: AA->VA\n",
    "## D: IA->VA/AV\n",
    "## E: II->AV\n",
    "## F: AA->VV\n",
    "## G: IA->VV \n",
    "## H: II->VV\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete excessive extra files\n",
    "def del_useless():\n",
    "    '''\n",
    "    Delete the extra files\n",
    "    '''\n",
    "    for root, dirs, files in os.walk(os.getcwd()):\n",
    "        for file in files:\n",
    "            for j in ['status','GssOrb','LprOrb','LoProp','guessorb','xmldump','RasOrb','SpdOrb']:\n",
    "                if j in file:\n",
    "    #                 print(root,dirs,file)\n",
    "                    os.remove(os.path.join(root,file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When restarting a setr of calculations just clear everyting out\n",
    "def clean_dir():\n",
    "    for entry in os.scandir(path=os.getcwd()):\n",
    "        if entry.is_dir():\n",
    "            if entry.name=='Fock':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if entry.name=='hdf5':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if entry.name=='e2':\n",
    "                shutil.rmtree(entry.name)                \n",
    "            if entry.name=='Labels':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if entry.name=='Coords':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if 'dir' in entry.name:\n",
    "                shutil.rmtree(entry.name)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before clean_dir, this pulls the xyz files out just to \n",
    "def pull_xyz():\n",
    "    import re\n",
    "    for i in struct_name:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),i))==False and os.path.exists(os.path.join(os.getcwd(),'Coords')):\n",
    "            shutil.copy(os.path.join(os.getcwd(),'/'.join(('Coords',i))),os.path.join(os.getcwd(),i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gateway(name,basis_set):\n",
    "    string=f'''&GATEWAY \n",
    "coord={f'{name}.xyz'}\n",
    "Basis = {basis_set}\n",
    "Group = nosymm\n",
    "Expert\n",
    "End of Input\n",
    "'''\n",
    "    return string\n",
    "\n",
    "def gen_seward():\n",
    "    string=f'''&SEWARD\n",
    "End of Input\n",
    "'''\n",
    "    return string\n",
    "\n",
    "def gen_motra(name):\n",
    "    string=f'''&MOTRA\n",
    "Frozen=0\n",
    "LUMORB\n",
    ">>> COPY $WorkDir/GMJ_one_int_indx.csv $CurrDir/{name}.GMJ_one_int_indx.csv\n",
    ">>> COPY $WorkDir/GMJ_one_int.csv $CurrDir/{name}.GMJ_one_int.csv\n",
    ">>> COPY $WorkDir/GMJ_two_int_indx.csv $CurrDir/{name}.GMJ_two_int_indx.csv\n",
    ">>> COPY $WorkDir/GMJ_two_int.csv $CurrDir/{name}.GMJ_two_int.csv\n",
    "'''\n",
    "    return string\n",
    "\n",
    "def gen_scf(name):\n",
    "    string=f\"\"\"&SCF &END\n",
    ">>> COPY $WorkDir/{name}.scf.h5 $CurrDir/\n",
    "\"\"\"\n",
    "    return string    \n",
    "\n",
    "\n",
    "def gen_rasscf(name,e,o,i,previous=None):\n",
    "    start_string=\"\"\"&RASSCF &END\n",
    "Title= RASSCF\n",
    "\"\"\"\n",
    "    if previous!=None:\n",
    "        fileorb=f\"\"\"FileOrb\n",
    "{previous}\n",
    "\"\"\"\n",
    "    else:\n",
    "        fileorb=''\n",
    "\n",
    "    end_string=f\"\"\"\n",
    "NACTEL\n",
    "{e} 0 0\n",
    "Inactive\n",
    "{i}\n",
    "RAS2\n",
    "{o}\n",
    "Symmetry\n",
    "1\n",
    "Spin\n",
    "1\n",
    "orblisting\n",
    "all\n",
    "ITERation\n",
    "200 100\n",
    "CIMX\n",
    "200\n",
    "SDAV\n",
    "500\n",
    "\n",
    ">>> COPY $WorkDir/{name}.rasscf.h5 $CurrDir/\n",
    ">>> COPY $WorkDir/GMJ_Fock_MO.csv $CurrDir/{name}.GMJ_Fock_MO.csv\n",
    "\"\"\"\n",
    "    return start_string+fileorb+end_string \n",
    "\n",
    "def gen_caspt2():\n",
    "    string=\"\"\"&CASPT2 &END\n",
    "Frozen \n",
    "0\n",
    "Imaginary Shift\n",
    "0.0\n",
    "\n",
    ">>foreach i in (B,E,F,G,H)\n",
    ">>foreach j in (P,M)\n",
    ">>if ( -FILE GMJ_e2_${i}_${j}.csv )\n",
    ">>> COPY $WorkDir/GMJ_RHS_${i}_${j}.csv $CurrDir/GMJ_RHS_${i}_${j}.csv\n",
    ">>> COPY $WorkDir/GMJ_IVECW_${i}_${j}.csv $CurrDir/GMJ_IVECW_${i}_${j}.csv\n",
    ">>> COPY $WorkDir/GMJ_e2_${i}_${j}.csv $CurrDir/GMJ_e2_${i}_${j}.csv\n",
    ">>endif\n",
    ">>enddo\n",
    ">>enddo\n",
    "\n",
    ">>foreach i in (A,C,D)\n",
    ">>if ( -FILE GMJ_e2_$i.csv )\n",
    ">>> COPY $WorkDir/GMJ_RHS_$i.csv $CurrDir/GMJ_RHS_$i.csv\n",
    ">>> COPY $WorkDir/GMJ_IVECW_$i.csv $CurrDir/GMJ_IVECW_$i.csv\n",
    ">>> COPY $WorkDir/GMJ_e2_$i.csv $CurrDir/GMJ_e2_$i.csv\n",
    ">>endif\n",
    ">>enddo\n",
    "\n",
    "\"\"\"\n",
    "    return string    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_set='ANO-RCC-MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top='/home/grierjones/DDCASPT2/hydrogen_comps/minimal_2e_2o/chains/even'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_range=np.linspace(0.6,3,100)\n",
    "chains=np.arange(2,14,2)\n",
    "# chains=[2]\n",
    "\n",
    "# chains=np.arange(2,10,2)\n",
    "\n",
    "\n",
    "\n",
    "# train_ind,test_ind=train_test_split(radius_range, test_size=0.3, random_state=0)\n",
    "# print(len(train_ind),len(test_ind))\n",
    "# with open('train_ind.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('test_ind.pickle', 'wb') as handle:\n",
    "#     pickle.dump(test_ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_ind.pickle', 'rb') as handle:\n",
    "    test_ind = pickle.load(handle)\n",
    "\n",
    "with open('train_ind.pickle', 'rb') as handle:\n",
    "    train_ind = pickle.load(handle)\n",
    "    \n",
    "print(len(train_ind),len(test_ind))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('/home/grierjones/DDCASPT2/hydrogen_comps/minimal_2e_2o/chains/even')\n",
    "# gen_data(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(i):\n",
    "    dirname=f'H{i}_chain'\n",
    "    print(dirname)\n",
    "    if os.path.exists(dirname)==False:\n",
    "        os.mkdir(dirname)\n",
    "        \n",
    "        \n",
    "    for idxr, r in enumerate(radius_range):\n",
    "        \n",
    "        # Loop radius\n",
    "        name=f\"H{i}_{r:.2f}\"\n",
    "\n",
    "        # Create files\n",
    "        if os.path.exists(os.path.join(dirname,f'{name}'))==False:\n",
    "            os.mkdir(os.path.join(dirname,f'{name}'))\n",
    "\n",
    "        # Write xyz\n",
    "        with open(os.path.join(dirname,f'{name}',f'{name}.xyz'),'w') as f:\n",
    "            f.write(f'{i}\\n\\n')\n",
    "            for j in range(i):\n",
    "                f.write(f'H {0:>8f} {0:>8f} {j*r:>8f}\\n')\n",
    "\n",
    "        # Write input\n",
    "        with open(os.path.join(dirname,f'{name}',f'{name}.input'),'wb') as g:\n",
    "            g.write(gen_gateway(name,basis_set).encode())\n",
    "            g.write(gen_seward().encode())\n",
    "            g.write(gen_scf(name).encode())   \n",
    "            # Choose active space and inactive orbitals\n",
    "            #g.write(gen_rasscf(name,2,2,int((i/2)-1)).encode())\n",
    "            if idxr==0:\n",
    "                g.write(gen_rasscf(name,i,i,0,previous=None).encode()) # int((i/2)-1)\n",
    "            else:\n",
    "\n",
    "                previous=os.path.join(top,dirname,f'H{i}_{radius_range[idxr-1]:.2f}',f\"H{i}_{radius_range[idxr-1]:.2f}.RasOrb\")\n",
    "                g.write(gen_rasscf(name,i,i,0,previous=previous).encode()) # int((i/2)-1)\n",
    "            g.write(gen_motra(name).encode())\n",
    "            g.write(gen_caspt2().encode())\n",
    "\n",
    "        # Change dir\n",
    "        if os.getcwd()!=os.path.join(dirname,f'{name}'):    \n",
    "            os.chdir(os.path.join(dirname,f'{name}'))\n",
    "\n",
    "        # Run\n",
    "        call(['pymolcas','-new','-clean',f'{name}.input', '-oe', f'{name}.output'])\n",
    "\n",
    "        # Back to top dir\n",
    "        if os.getcwd()!=top:\n",
    "            os.chdir(top)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# os.chdir('/home/grierjones/DDCASPT2/hydrogen_comps/minimal_2e_2o/chains/even')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# results = Parallel(n_jobs=16,verbose=1000)(delayed(gen_data)(i) for i in chains)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_energy():\n",
    "    chain_E={}\n",
    "    drop=[]\n",
    "    for i in chains:\n",
    "        energy=[]\n",
    "        dirname=f'H{i}_chain'\n",
    "        # Loop radius\n",
    "        for idr,r in enumerate(radius_range):\n",
    "            name=f\"H{i}_{r:.2f}\"\n",
    "            try:\n",
    "                output=os.path.join(dirname,f'{name}',f'{name}.output')\n",
    "                energy.append([r,float((grep['-i', '::    CASPT2',output] | awk['{print $NF }'])())])\n",
    "            except:\n",
    "                energy.append([r,0])\n",
    "                drop.append(idr)\n",
    "        chain_E[i]=np.array(energy)\n",
    "\n",
    "    for i in chains:\n",
    "        chain_E[i]=chain_E[i][~np.in1d(range(len(chain_E[i])),drop)]    \n",
    "        pd.DataFrame(chain_E[i],columns=['radius','energy']).to_csv(f'H{i}_chain/CASPT2.csv')\n",
    "        \n",
    "\n",
    "    casscf_chain_E={}\n",
    "    for i in chains:\n",
    "        casscf_energy=[]\n",
    "        dirname=f'H{i}_chain'\n",
    "        # Loop radius\n",
    "        for idr,r in enumerate(radius_range):\n",
    "            name=f\"H{i}_{r:.2f}\"\n",
    "            try:\n",
    "                output=os.path.join(dirname,f'{name}',f'{name}.output')\n",
    "                casscf_energy.append([r,float((grep['-i', '::    RASSCF root number  1',output] | awk['{print $8 }'])())])\n",
    "            except:\n",
    "                casscf_energy.append([r,0])\n",
    "        casscf_chain_E[i]=np.array(casscf_energy)\n",
    "\n",
    "    for i in chains:\n",
    "        casscf_chain_E[i]=casscf_chain_E[i][~np.in1d(range(len(casscf_chain_E[i])),drop)]    \n",
    "        pd.DataFrame(casscf_chain_E[i],columns=['radius','energy']).to_csv(f'H{i}_chain/CASSCF.csv')        \n",
    "        \n",
    "\n",
    "    E2_chain_E={}\n",
    "    for i in chains:\n",
    "        E2_energy=[]\n",
    "        dirname=f'H{i}_chain'\n",
    "        # Loop radius\n",
    "        for idr,r in enumerate(radius_range):\n",
    "            name=f\"H{i}_{r:.2f}\"\n",
    "            try:\n",
    "                output=os.path.join(dirname,f'{name}',f'{name}.output')\n",
    "                E2_energy.append([r,float((grep['-i', 'E2 (Variational):',output] | awk['{print $NF }'])())])\n",
    "            except:\n",
    "                E2_energy.append([r,0])\n",
    "        E2_chain_E[i]=np.array(E2_energy)\n",
    "\n",
    "    for i in chains:\n",
    "        E2_chain_E[i]=E2_chain_E[i][~np.in1d(range(len(E2_chain_E[i])),drop)]    \n",
    "        pd.DataFrame(E2_chain_E[i],columns=['radius','energy']).to_csv(f'H{i}_chain/E2.csv')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap=sns.color_palette('rocket',7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_energy()\n",
    "fig,ax=plt.subplots(2,2,figsize=(10,6),sharex=True)\n",
    "for idx,i in enumerate(chains):\n",
    "    CASSCF=pd.read_csv(f\"H{i}_chain/CASSCF.csv\",index_col=0).to_numpy()\n",
    "    CASPT2=pd.read_csv(f\"H{i}_chain/CASPT2.csv\",index_col=0).to_numpy()\n",
    "    if i<=4:\n",
    "        ax[0,idx%2].plot(CASSCF[:,0],CASSCF[:,1],color=cmap[idx],label='CASSCF')\n",
    "        ax[0,idx%2].plot(CASPT2[:,0],CASPT2[:,1],'--',color=cmap[idx],label='CASPT2')\n",
    "        ax[0,idx%2].legend()\n",
    "        ax[0,idx%2].set_title(f\"H$_{i}$\")\n",
    "        ax[0,idx%2].set_xlabel(\"Radius (Å)\")\n",
    "        ax[0,idx%2].set_ylabel(\"Energy (E$_{h}$)\")\n",
    "    else:\n",
    "        ax[1,idx%2].plot(CASSCF[:,0],CASSCF[:,1],color=cmap[idx],label='CASSCF')\n",
    "        ax[1,idx%2].plot(CASPT2[:,0],CASPT2[:,1],'--',color=cmap[idx],label='CASPT2')\n",
    "        ax[1,idx%2].legend()\n",
    "        ax[1,idx%2].set_title(\"H$_{\"+str(i)+\"}$\")\n",
    "        ax[1,idx%2].set_xlim(0.5,3.1)\n",
    "        ax[1,idx%2].set_xticks(np.round(np.linspace(min(radius_range),max(radius_range),4),2))\n",
    "        ax[1,idx%2].set_xlabel(\"Radius (Å)\")\n",
    "        ax[1,idx%2].set_ylabel(\"Energy (E$_{h}$)\")        \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('energies.png',dpi=300,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,i in enumerate(chains):\n",
    "    CASSCF=pd.read_csv(f\"H{i}_chain/CASSCF.csv\",index_col=0).to_numpy()\n",
    "    CASPT2=pd.read_csv(f\"H{i}_chain/CASPT2.csv\",index_col=0).to_numpy()\n",
    "    plt.plot(CASSCF[:,0],CASSCF[:,1],color=cmap[idx],label=f'CASSCF H$_{i}$')\n",
    "    plt.plot(CASPT2[:,0],CASPT2[:,1],'--',color=cmap[idx],label=f'CASPT2 H$_{i}$')    \n",
    "\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "#   Keep everything at float64\n",
    "DTYPE = float\n",
    "# DTYPE = np.float16\n",
    "\n",
    "#   Create an array with the easy data\n",
    "def createArrray(filename):\n",
    "    files = sorted(glob(filename))\n",
    "    arrayname = []\n",
    "    for i in sorted(files):\n",
    "        arrayname.append(\n",
    "            np.stack(\n",
    "                np.array(pd.read_csv(i, header=None),\n",
    "                         dtype=DTYPE,\n",
    "                         copy=False).flatten()))\n",
    "\n",
    "    arrayname = np.asarray(arrayname, dtype=DTYPE)\n",
    "    return arrayname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_all(name):\n",
    "    #   Start transforming the HDF5 files from the data directory\n",
    "    h5list = sorted(glob(f'{name}_chain/*/*rasscf.h5'))\n",
    "    f = h5.File(h5list[0], 'r')\n",
    "    datasetNames = [n for n in f.keys()]\n",
    "    b = []\n",
    "    labels = []\n",
    "    \n",
    "    # Useful attributes from the hdf5 files\n",
    "    NBAS=[]\n",
    "    NACTEL=[]\n",
    "    for k, elem in enumerate(h5list):\n",
    "        for count, ele in enumerate([i for i in f.attrs]):\n",
    "            if ele =='NBAS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r').attrs[ele]).reshape(-1)):\n",
    "                    NBAS.append(elemt)\n",
    "            if ele =='NACTEL':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r').attrs[ele]).reshape(-1)):\n",
    "                    NACTEL.append(elemt)\n",
    "    \n",
    "    \n",
    "    MO_ENERGIES=[]\n",
    "    MO_OCCUPATIONS=[]\n",
    "    MO_TYPEINDICES=[]\n",
    "    MO_VECTORS=[]\n",
    "    t0=time()\n",
    "    #   Eliminate certain features that won't be good for regression\n",
    "    for k, elem in enumerate(h5list):\n",
    "        for count, ele in enumerate([n for n in h5.File(elem, 'r').keys()]):\n",
    "            if ele =='MO_TYPEINDICES':\n",
    "                for i, elemt in enumerate(np.array(h5.File(elem,'r')[ele]).reshape(-1)):\n",
    "                    MO_TYPEINDICES.append(elemt)\n",
    "    \n",
    "            if ele =='MO_ENERGIES':\n",
    "                for i, elemt in enumerate(np.array(h5.File(elem,'r')[ele]).reshape(-1)):\n",
    "                    MO_ENERGIES.append(elemt)\n",
    "    \n",
    "            if ele =='MO_OCCUPATIONS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(elem,'r')[ele]).reshape(-1)):\n",
    "                    MO_OCCUPATIONS.append(elemt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'time: {time()-t0} s')\n",
    "    shape=len(h5list),int(NBAS[0])\n",
    "    # AO_FOCKINT_MATRIX=np.array(AO_FOCKINT_MATRIX).reshape(len(dislist),int(NBAS[0]),int(NBAS[0]))\n",
    "    MO_ENERGIES= np.array(MO_ENERGIES).reshape(shape)\n",
    "    MO_OCCUPATIONS= np.array(MO_OCCUPATIONS).reshape(shape)\n",
    "    MO_TYPEINDICES=np.array(MO_TYPEINDICES).reshape(shape)\n",
    "    \n",
    "    \n",
    "    h5list_scf = sorted(glob(f'{name}_chain/*/*.scf.h5'))\n",
    "    f = h5.File(h5list_scf[0], 'r')\n",
    "    datasetNames = [n for n in f.keys()]\n",
    "    b = []\n",
    "    labels = []\n",
    "    # AO_FOCKINT_MATRIX=[]\n",
    "    # Useful attributes from the hdf5 files\n",
    "    NBAS=[]\n",
    "    NACTEL=[]\n",
    "    for k, elem in enumerate(h5list_scf):\n",
    "        for count, ele in enumerate([i for i in f.attrs]):\n",
    "            if ele =='NBAS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r').attrs[ele]).reshape(-1)):\n",
    "                    NBAS.append(elemt)\n",
    "    MO_VECTORS=[]\n",
    "    scf_F=[]  \n",
    "    scf_OCC=[]\n",
    "    t0=time()\n",
    "    #   Eliminate certain features that won't be good for regression\n",
    "    for k, elem in enumerate(h5list_scf):\n",
    "        for count, ele in enumerate([n for n in h5.File(h5list_scf[k], 'r').keys()]):\n",
    "            if ele =='MO_VECTORS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_VECTORS.append(elemt)\n",
    "            if ele =='MO_ENERGIES':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r')[ele]).reshape(-1)):\n",
    "                    scf_F.append(elemt)\n",
    "            if ele =='MO_OCCUPATIONS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r')[ele]).reshape(-1)):\n",
    "                    scf_OCC.append(elemt)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(f'time: {time()-t0} s')\n",
    "    scf_F= np.array(MO_ENERGIES).reshape(shape)\n",
    "    scf_OCC= np.array(MO_OCCUPATIONS).reshape(shape)\n",
    "    MO_VECTORS=np.array(MO_VECTORS).reshape(len(h5list),int(NBAS[0]),int(NBAS[0]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    typ_exists=sorted(sum(list([j.replace('GMJ_e2_','') for j in i.split('/')[-1].split('.') if 'GMJ' in j] for i in glob(f'{name}_chain/{name}_{radius_range[0]:.2f}/GMJ_e2_*.csv')),[]))\n",
    "    dims_dict=gen_dim_dict(f'{name}_chain/{name}_{radius_range[0]:.2f}/',typ_exists)\n",
    "    \n",
    "    paths=glob(f'{name}_chain/{name}_*/')\n",
    "    path=paths[0]\n",
    "    # Generate the data\n",
    "    for typ in set([i.split('_')[0] for i in typ_exists ]):\n",
    "        if typ=='A':\n",
    "            typA_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typA_labels=stack_label(path,typ,typ_exists)\n",
    "            \n",
    "        if typ=='B':        \n",
    "            typB_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typB_labels=stack_label(path,typ,typ_exists)\n",
    "            \n",
    "        if typ=='C':\n",
    "            typC_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typC_labels=stack_label(path,typ,typ_exists)\n",
    "            \n",
    "        if typ=='D':        \n",
    "            typD_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typD_labels=stack_label(path,typ,typ_exists)\n",
    "                    \n",
    "        if typ=='E':\n",
    "            typE_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typE_labels=stack_label(path,typ,typ_exists)\n",
    "                    \n",
    "        if typ=='F':        \n",
    "            typF_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typF_labels=stack_label(path,typ,typ_exists)\n",
    "                    \n",
    "        if typ=='G':        \n",
    "            typG_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typG_labels=stack_label(path,typ,typ_exists)\n",
    "                    \n",
    "        if typ=='H':  \n",
    "            typH_e2=stack_e2(paths,typ,typ_exists)\n",
    "            typH_labels=stack_label(path,typ,typ_exists)\n",
    "    stacked_e2=pd.concat([gen_e2(paths,typ) for typ in typ_exists]).groupby(level=0).sum()\n",
    "    E2Dict=pd.read_csv(f\"{name}_chain/E2.csv\",index_col=0).to_numpy()\n",
    "    # stacked_e2.columns=[float(i.split('/')[1].split('_')[1]) for i in stacked_e2.columns]\n",
    "    stacked_e2=stacked_e2.sum(axis=0).sort_index().reset_index().to_numpy()\n",
    "    stacked_pairs=pd.concat([stack_e2(paths,typ,typ_exists) for typ in typ_exists]).groupby(level=0).sum()\n",
    "    pair_labels=stacked_pairs.index.tolist()\n",
    "    dummy_stack=pd.concat([gen_e2(paths,typ) for typ in typ_exists])\n",
    "    \n",
    "    \n",
    "    path_check=f'{name}_chain/{name}_{radius_range[0]:.2f}/{name}_{radius_range[0]:.2f}.output'\n",
    "    \n",
    "    # Sanity check...\n",
    "    # REMEVDZPER FROZEN CORE APPROXIMATION\n",
    "    # Number of frozen orbitals\n",
    "    fro=int(subprocess.Popen(f\"grep -i 'Frozen orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "    # Number of inactive orbitals\n",
    "    inact=int(subprocess.Popen(f\"grep -i 'Inactive orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "    # Number of active orbitals\n",
    "    act=int(subprocess.Popen(f\"grep -i 'Active orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "    # Number of seconary orbitals\n",
    "    virt=int(subprocess.Popen(f\"grep -i 'Secondary orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "    # Number of basis functions for sanity check\n",
    "    bas_check=int(subprocess.Popen(f\"grep -i 'Number of basis functions' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "    \n",
    "    Basis_Indices=[]\n",
    "    for i in range(fro):\n",
    "        Basis_Indices.append(f'F{i+1}')\n",
    "    for i in range(inact):\n",
    "        Basis_Indices.append(f'I{i+1}')\n",
    "    for i in range(act):\n",
    "        Basis_Indices.append(f'A{i+1}')\n",
    "    for i in range(virt):\n",
    "        Basis_Indices.append(f'S{i+1}')    \n",
    "        \n",
    "    print(f'Basis sanity check passed={bas_check==len(Basis_Indices)}') \n",
    "    \n",
    "    # Grab molecular orbital occupations and make it into a dataframe labeled with xyz file name\n",
    "    MO_OCC=[]\n",
    "    for j in range(len(chains)):\n",
    "        MO_OCC.append(dict(zip(Basis_Indices,[i for i in list(MO_OCCUPATIONS[j])])))\n",
    "    MO_OCC_Dict=dict(zip([str(k) for k in paths],MO_OCC))\n",
    "    MO_OCC_DF=pd.DataFrame(MO_OCC_Dict)\n",
    "    \n",
    "    # Dataframe of MO occupation, index=basis indices and columns=paths\n",
    "    MO_OCCUPATIONS_DF=pd.DataFrame(MO_OCCUPATIONS,index=paths,columns=Basis_Indices).transpose()\n",
    "    \n",
    "    # \n",
    "    # Keep in mind HDF5 zeroes out the actrive orbitals... we'll use the Fock matrix to recover these\n",
    "    # \n",
    "    # Grab molecular orbital energy and make it into a dataframe labeled with xyz file name\n",
    "    MO_ENERGIES=[]\n",
    "    for j in paths:\n",
    "        MO_ENERGIES.append(np.genfromtxt(os.path.join(j,f\"{j.split('/')[1]}.GMJ_Fock_MO.csv\"), delimiter=''))\n",
    "    \n",
    "    \n",
    "    # Dataframe of MO energies, index=basis indices and columns=paths\n",
    "    MO_ENERGIES_DF=pd.DataFrame(MO_ENERGIES,index=paths,columns=Basis_Indices).transpose()\n",
    "    t0=time()\n",
    "    int1=gen_one_int(paths,Basis_Indices)\n",
    "    print(f'Integrals loaded in {time()-t0:0.4f} s')\n",
    "    # pd.set_option(\"precision\", 2)\n",
    "    # np.set_printoptions(precision=2)\n",
    "    # pd.options.display.float_format = '{:,.2f}'.format\n",
    "    \n",
    "    \n",
    "    \n",
    "    nmo=len(Basis_Indices)\n",
    "    indice=[]\n",
    "    ad_ind=[]\n",
    "    for ind,i in enumerate(range(nmo)):\n",
    "        for indx,j in enumerate(range(nmo)):\n",
    "            ad_ind.append(f'{i+1}_{j+1}')\n",
    "            if j<=i:\n",
    "                indice.append(f'{i+1}_{j+1}')\n",
    "    \n",
    "    \n",
    "    froz=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('F')]\n",
    "    inact=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('I')]\n",
    "    act=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('A')]\n",
    "    virt=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('S')]\n",
    "    # gen_F=dict(zip(paths,scf_F))\n",
    "    # gen_occ=dict(zip(paths,scf_OCC))\n",
    "    \n",
    "    gen_F=MO_ENERGIES_DF\n",
    "    gen_occ=MO_OCCUPATIONS_DF\n",
    "    gen_F_SCF=pd.DataFrame(scf_F,columns=Basis_Indices,index=paths).T\n",
    "    gen_occ_SCF=pd.DataFrame(scf_OCC,columns=Basis_Indices,index=paths).T\n",
    "    \n",
    "    \n",
    "    full_set=sorted(set(sum([gen_pair_labels(path,typ) for typ in typ_exists],[])))\n",
    "    \n",
    "    # Rewrite the jacob style featurization step\n",
    "    occcc=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=0])\n",
    "    virttt=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=2])\n",
    "    \n",
    "    \n",
    "    argged=np.argsort(dummy_stack.abs().values,axis=0).T\n",
    "    top_excits={}\n",
    "    for idxc,c in enumerate(dummy_stack.columns):\n",
    "        top_excits[c]=dummy_stack[c].iloc[argged[idxc]].iloc[-4:].index\n",
    "    \n",
    "    ijkl_idx=gen_big_4().gen_ijkl(Basis_Indices,stacked_pairs,dummy_stack)\n",
    "    set_i_indices=ijkl_idx[0]\n",
    "    set_j_indices=ijkl_idx[1]\n",
    "    set_k_indices=ijkl_idx[2]\n",
    "    set_l_indices=ijkl_idx[3]\n",
    "    set_indices=ijkl_idx[4]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    occcc=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=0])\n",
    "    virttt=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=2])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create slices for AA->AA indices\n",
    "    # This will help zero out infinities\n",
    "    internal_A=[idx for idx,i in enumerate(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=0].index) if 'A' in i]\n",
    "    external_A=[idx for idx,i in enumerate(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=2].index) if 'A' in i]\n",
    "    \n",
    "    inner_slice=slice(min(internal_A),max(internal_A)+1)\n",
    "    outer_slice=slice(min(external_A),max(external_A)+1)\n",
    "    \n",
    "    Big_Data_GS(name,paths,full_set,virttt,occcc,froz,Basis_Indices,gen_F,gen_occ,gen_F_SCF,gen_occ_SCF,pair_labels,inner_slice,outer_slice,set_indices,set_i_indices,set_j_indices,set_k_indices,set_l_indices,int1,stacked_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the labels that match the IVECW and IVECC2 files\n",
    "def gen_labels(path,typ):\n",
    "    return [re.sub(r'(?<!\\d)0+(\\d+)', r'\\1', j) for j in pd.read_csv(f'{path}/GMJ_RHS_{typ}.csv',header=None)[0]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair_labels(path,typ):\n",
    "    Labels=[]\n",
    "    Indexes=[]\n",
    "    return sorted(set(['_'.join(re.sub(r'(?<!\\d)0+(\\d+)', r'\\1', j).split('_')[0:2]) for j in pd.read_csv(f'{path}/GMJ_RHS_{typ}.csv',header=None)[0].values]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path,typ = \"H10_chain/H10_0.60/\",\"C\"\n",
    "# gen_pair_labels(path,typ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dim_dict(path,typ_exists):\n",
    "    '''    \n",
    "    Dimension check for DDCASPT2: check the ordering of the pair-energies,\n",
    "    this notation follows a mix of the papers and code.\n",
    "    \n",
    "    A (IA->AA): \\n TIUV \\n E_{ti} E_{uv} \\n pqrs=tiuv=0123 \\n    \n",
    "    B_P (II->AA) (P): \\n IJTU \\n E_{ti} E_{uj} \\n pqrs=tiuj=2031 \\n\n",
    "    B_M (II->AA) (M): \\n IJTU \\n E_{ti} E_{uj} \\n pqrs=tiuj=2031 \\n\n",
    "    C (AA->VA): \\n UVAT \\n E_{at} E_{uv} \\n pqrs=atuv=2301 \\n\n",
    "    D (IA->VA/AV): \\n IUAT/IUTA \\n E_{ai} E_{tu}/E_{ti} E_{au} \\n pqrs=(a/t)i(t/a)u=2031 \\n\n",
    "    E_P (II->AV) (P): \\n IJAT \\n E_{ti} E_{aj} \\n pqrs=tiaj=3021 \\n\n",
    "    E_M (II->AV) (M): \\n IJAT \\n E_{ti} E_{aj} \\n pqrs=tiaj=3021 \\n\n",
    "    F_P (AA->VV) (P): \\n TUAB \\n E_{at} E_{bu} \\n pqrs=atbu=2031 \\n\n",
    "    F_M (AA->VV) (M): \\n TUAB \\n E_{at} E_{bu} \\n pqrs=atbu=2031 \\n\n",
    "    G_P (IA->VV) (P): \\n ITAB \\n E_{ai} E_{bt} \\n pqrs=aibt=2031 \\n\n",
    "    G_M (IA->VV) (M): \\n ITAB \\n E_{ai} E_{bt} \\n pqrs=aibt=2031 \\n\n",
    "    H_P (II->VV) (P): \\n IJAB \\n E_{ai} E_{bj} \\n pqrs=aibj=2031 \\n\n",
    "    H_M (II->VV) (M): \\n IJAB \\n E_{ai} E_{bj} \\n pqrs=aibj=2031 \\n\n",
    "    '''    \n",
    "    dims=[]\n",
    "    for typ in typ_exists:\n",
    "        dims.append((typ,np.array([i.split('=')[-1].split('x') for i in open(os.path.join(f'{path}',f'GMJ_e2_{typ}.csv'),'r').readlines() if 'mat. size =' in i ]).flatten().astype(int)))\n",
    "    return dict(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(lst):   \n",
    "    return '_'.join(i.replace('A00','A').replace('I00','I').replace('S00','S').replace('I0','I').replace('A0','A').replace('S0','S') for i in lst.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ordered(path,typ):\n",
    "    '''\n",
    "    Return a dataframe for each type\n",
    "    Index=proper indexing\n",
    "    level_0=row\n",
    "    level_1=column\n",
    "    0=W value\n",
    "    '''\n",
    "    ordered=pd.read_csv(os.path.join(path,f'GMJ_IVECW_{typ}.csv'),sep='\\s+', skiprows=[0],header=None).astype(np.float64).dropna(axis=1)\n",
    "    \n",
    "    ordered.columns=list(range(len(ordered.columns)))\n",
    "    \n",
    "    ordered=ordered.stack()\n",
    "    \n",
    "    df=pd.read_csv(os.path.join(path,f'GMJ_RHS_{typ}.csv'),header=None,delimiter=',',index_col=0)\n",
    "    df.index=list(map(strip,df.index))\n",
    "    merged=ordered.reset_index().sort_values(by=0).set_index(df.sort_values(by=1).index).sort_values(['level_0','level_1'])    \n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate IVECW\n",
    "def gen_e2(paths,typ):\n",
    "    e2=[]\n",
    "    \n",
    "    for i in paths:\n",
    "        proper_labels=gen_labels(i,typ)\n",
    "        df=pd.read_csv(os.path.join(i,f'GMJ_e2_{typ}.csv'),sep='\\s+', skiprows=[0],header=None).astype(np.float64).dropna(axis=1).stack()\n",
    "        df.index=gen_ordered(i,typ).index\n",
    "        df=df.to_frame(name=str(i.split('/')[1].split('_')[1]))\n",
    "        e2.append(df)\n",
    "    df1=pd.concat(e2,axis=1).loc[proper_labels]\n",
    "    df1.index=[i for idx,i in enumerate(proper_labels)]\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(paths,typ):\n",
    "    Y=gen_e2(paths,typ).astype(float)\n",
    "# Needs to be qs, we're summing over the occupied orbitals    \n",
    "    Y_pair_set=list(set(['_'.join((i.split('_')[0],i.split('_')[1]))+'_' for i in Y.index.tolist()]))\n",
    "    Y_pair_df=pd.concat([Y[Y.index.str.find(j)==0].sum() for j in Y_pair_set],axis=1)\n",
    "    Y_pair_df.columns=list(set(['_'.join((i.split('_')[0],i.split('_')[1])) for i in Y.index.tolist()]))\n",
    "    return Y_pair_df.T.sort_index().groupby(level=0).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_label(path,typ,typ_exists):\n",
    "    if f'{typ}_M' in typ_exists and f'{typ}_P' in typ_exists:\n",
    "        return gen_pair_labels(path,f'{typ}_P')+gen_pair_labels(path,f'{typ}_M')\n",
    "    elif f'{typ}_P' in typ_exists:\n",
    "        return gen_pair_labels(path,f'{typ}_P')\n",
    "    elif f'{typ}_M' in typ_exists:\n",
    "        return gen_pair_labels(path,f'{typ}_M')\n",
    "    elif f'{typ}' in typ_exists:\n",
    "        return gen_pair_labels(path,f'{typ}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name='H2'\n",
    "# typ_exists=sorted(sum(list([j.replace('GMJ_e2_','') for j in i.split('/')[-1].split('.') if 'GMJ' in j] for i in glob(f'{name}_chain/{name}_{radius_range[0]:.2f}/GMJ_e2_*.csv')),[]))\n",
    "# dims_dict=gen_dim_dict(f'{name}_chain/{name}_{radius_range[0]:.2f}/',typ_exists)\n",
    "# stack_label(glob(f'{name}_chain/{name}_*/')[0],'C',typ_exists)\n",
    "# # gen_pair_labels(path, typ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_e2(path,typ,typ_exists):\n",
    "    if f'{typ}_M' in typ_exists and f'{typ}_P' in typ_exists:\n",
    "        df=pd.concat([gen_pair(path,f'{typ}_M'),gen_pair(path,f'{typ}_P')],axis=0).groupby(level=0).sum()\n",
    "        return df\n",
    "    elif f'{typ}_P' in typ_exists:\n",
    "        return gen_pair(path,f'{typ}_P').groupby(level=0).sum()\n",
    "    elif f'{typ}_M' in typ_exists:\n",
    "        return gen_pair(path,f'{typ}_M').groupby(level=0).sum()\n",
    "    elif f'{typ}' in typ_exists:\n",
    "        return gen_pair(path,f'{typ}').groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_indx(list_of_dicts):\n",
    "    indx=[]\n",
    "    for i in list_of_dicts.keys():\n",
    "        if len(list_of_dicts[i])>0:\n",
    "            indx.append(list(list_of_dicts[i].keys()))\n",
    "    return indx[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_one_int(paths,Basis_Indices):\n",
    "    one_int=[]\n",
    "    Labels=[]\n",
    "    Indexes=[]\n",
    "    upd_1int_indx=[]\n",
    "    def one_gener(i):\n",
    "        return pd.DataFrame(np.genfromtxt(os.path.join(i,f\"{i.split('/')[1]}.GMJ_one_int.csv\"), delimiter='',dtype=float),index=Basis_Indices,columns=Basis_Indices)\n",
    "            \n",
    "\n",
    "#     Dict=dict(zip(Indexes,Labels))\n",
    "    return dict((i,one_gener(i)) for i in paths)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def gen_MO(k,Basis_Indices):\n",
    "    '''\n",
    "    FCIDUMP/MOLCAS notation: \n",
    "    i j k l: <ik|jl>\n",
    "    swap index 1 and 2\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if os.path.exists(os.path.join(k,f\"{k.split('/')[1]}.GMJ_two_int.csv\"))==True:\n",
    "\n",
    "        raw_MO=np.genfromtxt(os.path.join(k,f\"{k.split('/')[1]}.GMJ_two_int.csv\")).reshape(len(Basis_Indices),len(Basis_Indices),len(Basis_Indices),len(Basis_Indices))\n",
    "        new_MO = np.copy(raw_MO)\n",
    "        new_MO[:,1]=raw_MO[:,2]\n",
    "        new_MO[:,2]=raw_MO[:,1]\n",
    "    return new_MO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMJ here\n",
    "initial = np.genfromtxt('H2_chain/H2_0.60/H2_0.60.GMJ_two_int.csv').reshape(10,10,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen_big_4(object):\n",
    "    '''\n",
    "    Generate a set of i,j,k,l indices corresponding to the largest 4 two-electron excitations \n",
    "    per pair-energy.\n",
    "\n",
    "    !!!From here on out we need to use the same training set and test set.!!!\n",
    "    Indices:\n",
    "    i-internal\n",
    "    j-internal\n",
    "    k-external\n",
    "    l-external\n",
    "\n",
    "    '''    \n",
    "    # Pick 4 largest contributers to the pair energy\n",
    "    def big_4(self,dist,dummy_stack,stacked_pairs):\n",
    "        return [['_'.join(k.split('_')[2:4]) for k in dummy_stack.loc[[ j for j in dummy_stack.index.tolist() if j.split('_')[0]==i.split('_')[0] and j.split('_')[1]==i.split('_')[1]]][dist].abs().sort_values(ascending=False).index[0:4].tolist()] for i in stacked_pairs.index.tolist()]\n",
    "    \n",
    "    def gen_ijkl(self,Basis_Indices,stacked_pairs,dummy_stack):\n",
    "        # Dimensions: training index x pair energies x top 4 most frequent virtual orbitals for two electrion excitations\n",
    "        \n",
    "        self.train_4=np.array(Parallel(n_jobs=-1,verbose=10)(delayed(self.big_4)(f'{i:.2f}',dummy_stack,stacked_pairs) for i in train_ind))\n",
    "        print(self.train_4.shape)\n",
    "        # Most frequent: list of tuples containing (pair energy index, [top 4 most frequent virtual orbitals for two electrion excitations])\n",
    "        # This can serve as a label too.\n",
    "        self.train_freq=[(i,pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()) for idx,i in enumerate(stacked_pairs.index.tolist())]\n",
    "        # i,j, set of 4 largest [(k,l)]\n",
    "        self.set_ijkl_indices=[(Basis_Indices.index(i.split('_')[0]),Basis_Indices.index(i.split('_')[1]),[(Basis_Indices.index(k.split('_')[0]),Basis_Indices.index(k.split('_')[1])) for k in pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()]) for idx,i in enumerate(stacked_pairs.index.tolist())]\n",
    "        internal_basis=[i for i in Basis_Indices if 'S' not in i]\n",
    "        external_basis=[i for i in Basis_Indices if 'I' not in i]\n",
    "        print(external_basis)\n",
    "    # This is 1 indices per pair energy!        \n",
    "        set_i_indices=[]\n",
    "    # This is 1 indices per pair energy!            \n",
    "        set_j_indices=[]\n",
    "    # This is a set of 4 indices per pair energy!    \n",
    "        set_k_indices=[]\n",
    "    # This is a set of 4 indices per pair energy!    \n",
    "        set_l_indices=[]\n",
    "        for idx,i in enumerate(stacked_pairs.index.tolist()):\n",
    "            set_i_indices.append(internal_basis.index(i.split('_')[0]))\n",
    "            set_j_indices.append(internal_basis.index(i.split('_')[1]))\n",
    "            set_k_indices.append([external_basis.index(k.split('_')[0]) for k in pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()])\n",
    "            set_l_indices.append([external_basis.index(k.split('_')[1]) for k in pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()])\n",
    "            \n",
    "        return set_i_indices,set_j_indices,set_k_indices,set_l_indices,self.train_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class gen_two_ints(object):\n",
    "# Some of the D_{ij}^{ab}=f_{ii}+f_{jj}-f_{aa}-f_{bb}=e_{ii}+e_{jj}-e_{aa}-e_{bb} elements \n",
    "# will be 0 since ij and ab overlap for CASPT2\n",
    "# ij \\in {I,A}\n",
    "# ab \\in {A,V}\n",
    "# So ignore the warnings since they'll only be 0 when ijab \\in {A} only\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    def get_MO(self, string):\n",
    "        return self.MO[self.slice_dict[string[0]], self.slice_dict[string[1]],self.slice_dict[string[2]], self.slice_dict[string[3]]]\n",
    "\n",
    "    def get_F(self, string):\n",
    "        return self.F[self.slice_dict[string[0]], self.slice_dict[string[1]]] \n",
    "    \n",
    "    def compute_pairmatrix(self,selft2start,selfdoublecheck):\n",
    "        test = 2*selft2start*selfdoublecheck\n",
    "        test -= np.swapaxes(selft2start,2,3)*selft2start\n",
    "        c=np.sum(test,axis=(2,3))\n",
    "        return c    \n",
    "    \n",
    "    def build_tau(self,t2,t1):\n",
    "        ttau = t2.copy()\n",
    "        tmp = np.einsum('ia,jb->ijab', t1, t1,optimize=True)\n",
    "        ttau += tmp\n",
    "        return ttau    \n",
    "    \n",
    "#     def __init__(self,k):\n",
    "    def gen_feat(self,k,virttt,occcc,froz,Basis_Indices,gen_F,inner_slice,outer_slice,set_indices,set_i_indices,set_j_indices,set_k_indices,set_l_indices):\n",
    "# Set variables\n",
    "# nocc:=Occupied orbitals\n",
    "# nvirt:=virtual orbitals\n",
    "# nfzc:=frozen orbitals\n",
    "# nmo:=number of orbitals\n",
    "        nocc=occcc\n",
    "        nvirt=virttt\n",
    "        self.nfzc=len(froz)\n",
    "        self.nocc=nocc\n",
    "        self.nvirt=nvirt\n",
    "        self.nmo=len(Basis_Indices)\n",
    "        nmo=nocc+nvirt\n",
    "# nmo=nocc+nvirt\n",
    "# Set slices\n",
    "# In CASPT2 occupied run I-A and virtual run A-S\n",
    "# Unlike MP2 there will be overlap between internal and external indices\n",
    "        self.slice_o = slice(0,nocc)\n",
    "        self.slice_v = slice(len(Basis_Indices)-virttt,len(Basis_Indices))\n",
    "        self.slice_a = slice(0,nmo)    \n",
    "\n",
    "# Dictionary with the slices        \n",
    "\n",
    "        self.slice_dict = {\n",
    "            'o': self.slice_o,\n",
    "            'v': self.slice_v,\n",
    "            'a': self.slice_a\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        featurelist=list()    \n",
    "# Virtual\n",
    "        self.empty=np.zeros((nvirt,))\n",
    "# Occupied    \n",
    "        self.occupado=np.zeros((nocc,))\n",
    "# MO integrals    \n",
    "        self.MO=gen_MO(k,Basis_Indices)\n",
    "        \n",
    "# MO Fock matrix    \n",
    "        self.F=gen_F[k].to_numpy()\n",
    "# Zero out t1 matrix, dim(occ,virt)    \n",
    "        self.t1=np.zeros((nocc,nvirt))\n",
    "# MO Fock matrix internal\n",
    "        Focc = self.F[self.slice_o]\n",
    "# MO Fock matrix external    \n",
    "        Fvir = self.F[self.slice_v]  \n",
    "        self.orbocc=Focc\n",
    "        self.orbvirt=Fvir\n",
    "\n",
    "        self.Dia = Focc.reshape(-1, 1) - Fvir\n",
    "        self.Dijab = Focc.reshape(-1, 1, 1, 1) + Focc.reshape(-1, 1, 1) - Fvir.reshape(-1, 1) - Fvir \n",
    "\n",
    "        \n",
    "# Clean up AA block to get rid of infinities, we do not need this anyway              \n",
    "# t2=<ij||ab>/(e_i+e_j-e_a-e_b)\n",
    "# But this is <ij|ab>... idk...\n",
    "        self.t2start=self.MO[self.slice_o, self.slice_o, self.slice_v,self.slice_v] / self.Dijab\n",
    "# Zero out AA->AA\n",
    "        # self.t2start[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "# Related to the correlation energy\n",
    "# np.einsum('ijab->',triplecheck)=MP2 correlation energy in jacob's code\n",
    "#dim(triplecheck)=(occ,occ,virt,virt)\n",
    "        self.triplecheck=2*self.t2start*self.get_MO('oovv')\n",
    "        self.triplecheck -=  np.swapaxes(self.get_MO('oovv'),2,3)*self.t2start \n",
    "# Zero out AA->AA\n",
    "        # self.triplecheck[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "        \n",
    "        \n",
    "# Integral\n",
    "# doublecheck =<ij|ab>\n",
    "#dim(doublecheck)=(occ,occ,virt,virt)\n",
    "        self.doublecheck = self.MO[self.slice_o, self.slice_o, self.slice_v, self.slice_v]   \n",
    "# Zero out AA->AA\n",
    "        # self.doublecheck[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "\n",
    "    \n",
    "# Not related to the correlation energy?\n",
    "# dim(pairenergy)=(occ,occ,virt,virt)\n",
    "# dim(compute_pairmatrix(t2start,doublecheck))=(occ,occ)->(occ,occ,virt,virt)\n",
    "        self.pairenergy=(np.zeros(self.doublecheck.shape)+self.compute_pairmatrix(self.t2start,self.doublecheck)[:,:,np.newaxis,np.newaxis])\n",
    "# Zero out AA->AA\n",
    "        # self.pairenergy[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "\n",
    "# Related to the correlation energy\n",
    "#dim(pairs)=(virt,virt)\n",
    "# np.einsum('ab->',pairs)=np.einsum('ijab->',triplecheck)=MP2 correlation energy in jacob's code\n",
    "        tmp_tau = self.build_tau(self.t2start,self.t1)\n",
    "        self.pairs=2*tmp_tau*self.get_MO('oovv')\n",
    "        self.pairs-= np.swapaxes(self.get_MO('oovv'),2,3)*tmp_tau\n",
    "        self.pairs = np.sum(self.pairs,axis=(2,3))\n",
    "# Zero out AA->AA\n",
    "        # self.pairs[inner_slice,outer_slice]=0        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        test=np.zeros(self.t2start.shape)\n",
    "        self.diag=test\n",
    "        for i in range (0,self.nocc):\n",
    "            for j in range (0,self.nocc):\n",
    "                np.fill_diagonal(self.diag[i,j,:,:],1)\n",
    "\n",
    "# Dim(temp)=(occ,virt)\n",
    "# Basically dim(<ii|jj>)->dim(<i|j>)\n",
    "        temp=np.zeros((self.nocc,self.nvirt))\n",
    "        for i in range (0,self.nocc):\n",
    "            for j in range (0,self.nvirt):\n",
    "                temp[i,j]=self.doublecheck[i,i,j,j]\n",
    "# Dim(test1)=(occ,occ,virt,virt)                \n",
    "        test1=np.zeros((self.t2start.shape))\n",
    "\n",
    "# dim(temp)=(occ,virt) -> dim(temp[:,np.newaxis,:,np.newaxis])=(occ,1,virt,1)\n",
    "# np.newaxis makes this a dummy index, : will be unique\n",
    "# i.e. (occ,copy,virt,copy)\n",
    "# (occ,occ,virt,virt) + (occ,1,virt,1)\n",
    "# <ii||aa>    \n",
    "        self.screen1=test1+temp[:,np.newaxis,:,np.newaxis]\n",
    "# Zero out AA->AA\n",
    "        # self.screen1[inner_slice,inner_slice,outer_slice,outer_slice]=0            \n",
    "\n",
    "# dim(temp)=(occ,virt) -> dim(temp[np.newaxis,:,np.newaxis,:])=(1,occ,1,virt)\n",
    "# (occ,occ,virt,virt) + (1,occ,1,virt)\n",
    "# np.newaxis makes this a dummy index, : will be unique\n",
    "# i.e. (copy,occ,copy,virt)\n",
    "# <jj||bb>    \n",
    "        self.screen2=test1+temp[np.newaxis,:,np.newaxis,:]\n",
    "# Zero out AA->AA\n",
    "        # self.screen2[inner_slice,inner_slice,outer_slice,outer_slice]=0                \n",
    "# screen1[i,j,k,l]==screen2[j,i,l,k].T\n",
    "\n",
    "# nfzc=# frozen core\n",
    "        val=self.nmo-self.nfzc\n",
    "# Dim(temp)=(nmo,nmo)\n",
    "# Basically dim(<ii|jj>)->dim(<i|j>)    \n",
    "        temp=np.zeros((val,val))        \n",
    "        for i in range (0,val):\n",
    "            for j in range (0,val):\n",
    "                temp[i,j]=self.MO[i,i,j,j]\n",
    "# Get virtual indices: <aa|bb> basically\n",
    "# <aa||bb>\n",
    "# (occ,occ,virt,virt) + (virt,virt)\n",
    "# (occ,occ,virt,virt) + (1,1,virt,virt)                \n",
    "        temp =temp[self.slice_v,self.slice_v]\n",
    "        self.screenvirt=test1+temp[np.newaxis,np.newaxis,:,:]\n",
    "# Zero out AA->AA\n",
    "        # self.screenvirt[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "        # b=(i,j,a,b)=(13, 13, 5, 5)\n",
    "        b=self.triplecheck\n",
    "# Zero out AA->AA\n",
    "        diag_indx=[]\n",
    "        off_diag_indx=[]     \n",
    "        featurelist=[]\n",
    "        featurelist.clear()\n",
    "        feature=[]\n",
    "        feature.clear()    \n",
    "        index=['pair_energy','coulomb','screen1_1','screen1_2','screen1_3','screen1_4','screen2_1','screen2_2','screen2_3','screen2_4','eijab_1','eijab_2','eijab_3','eijab_4','screenvirt_1','screenvirt_2','screenvirt_3','screenvirt_4']        \n",
    "        for idx,i in enumerate([j for j,v in set_indices]):\n",
    "# εij{MP2}              \n",
    "            new=np.sum(b[set_i_indices[idx],set_j_indices[idx]])#0\n",
    "# <ii||jj>    \n",
    "            new=np.hstack((new,self.MO[set_i_indices[idx],set_i_indices[idx],set_j_indices[idx],set_j_indices[idx]]))\n",
    "            # print('GMJ Coulomb <ii||jj>',self.MO[set_i_indices[idx],set_i_indices[idx],set_j_indices[idx],set_j_indices[idx]])\n",
    "# <ii||aa>    \n",
    "            new=np.hstack((new,np.array([self.screen1[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "# <jj||bb>    \n",
    "            new=np.hstack((new,np.array([self.screen2[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "# e_{ij}^{ab} MP2\n",
    "            new=np.hstack((new,np.array([b[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "# # <aa||bb>    \n",
    "            new=np.hstack((new,np.array([self.screenvirt[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "            featurelist.append((i,new))\n",
    "        return pd.DataFrame(dict(featurelist),index=index).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements(x):\n",
    "    '''\n",
    "    Takes an integer, x, and returns the number of off-diagonal elements of an upper triangular matrix\n",
    "    f(x)=(x*(x-1))/2\n",
    "    '''\n",
    "    return (x*(x-1))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1_feats(k,int1,set_indices,gen_F,gen_occ,gen_F_SCF,gen_occ_SCF,pair_labels):\n",
    "    # t0=time()\n",
    "    # For an e_pq, there should be 10 F, 10 occs, and then 10 SCF F and 10 SCF occs\n",
    "    Fp=[]\n",
    "    Fq=[]\n",
    "\n",
    "    occp=[]\n",
    "    occq=[]\n",
    "\n",
    "    SCFFp=[]\n",
    "    SCFFq=[]\n",
    "\n",
    "\n",
    "    SCFOCCp=[]\n",
    "    SCFOCCq=[]\n",
    "\n",
    "    hpp=[]\n",
    "    hqq=[]\n",
    "    hrr=[]\n",
    "    hss=[]\n",
    "\n",
    "    rs_df=[]\n",
    "    h=int1[k]\n",
    "    for idx,i in enumerate(set_indices):\n",
    "\n",
    "        p,q=i[0].split('_')\n",
    "        # print(p,q)\n",
    "        Fp.append(gen_F.loc[p,k])\n",
    "        Fq.append(gen_F.loc[q,k])\n",
    "        occp.append(gen_occ.loc[p,k])\n",
    "        occq.append(gen_occ.loc[q,k])\n",
    "\n",
    "        SCFFp.append(gen_F_SCF.loc[p,k])\n",
    "        SCFFq.append(gen_F_SCF.loc[q,k])\n",
    "        SCFOCCp.append(gen_occ_SCF.loc[p,k])\n",
    "        SCFOCCq.append(gen_occ_SCF.loc[q,k])\n",
    "        hpp.append(h.loc[p,p])\n",
    "        hqq.append(h.loc[p,p])\n",
    "\n",
    "\n",
    "\n",
    "        Fr=[]\n",
    "        Fs=[]\n",
    "        occr=[]\n",
    "        occs=[]\n",
    "        SCFFr=[]\n",
    "        SCFFs=[]\n",
    "        SCFOCCr=[]\n",
    "        SCFOCCs=[]\n",
    "        for idxx,j in enumerate(i[1]):\n",
    "            r,s=j.split('_')\n",
    "\n",
    "            Fr.append((f'Fr{idxx+1}',gen_F.loc[r,k]))\n",
    "            Fs.append((f'Fs{idxx+1}',gen_F.loc[s,k]))\n",
    "\n",
    "            occr.append((f'occr{idxx+1}',gen_occ.loc[r,k]))\n",
    "            occs.append((f'occs{idxx+1}',gen_occ.loc[s,k]))\n",
    "\n",
    "            SCFFr.append((f'SCFFr{idxx+1}',gen_F_SCF.loc[r,k]))\n",
    "            SCFFs.append((f'SCFFs{idxx+1}',gen_F_SCF.loc[s,k]))\n",
    "\n",
    "\n",
    "            SCFOCCr.append((f'SCFOCCr{idxx+1}',gen_occ_SCF.loc[r,k]))\n",
    "            SCFOCCs.append((f'SCFOCCs{idxx+1}',gen_occ_SCF.loc[s,k]))\n",
    "\n",
    "            hrr.append((f'hrr{idxx+1}',h.loc[r,r]))\n",
    "            hss.append((f'hss{idxx+1}',h.loc[s,s]))\n",
    "\n",
    "\n",
    "        rs_df.append(pd.DataFrame.from_dict({**dict(Fr),**dict(Fs),**dict(occr),**dict(occs),**dict(SCFFr),**dict(SCFFs),**dict(SCFOCCr),**dict(SCFOCCs),**dict(hrr),**dict(hss)},orient='index',columns=[i[0]]))\n",
    "    rs=pd.concat(rs_df,axis=1).T\n",
    "    dummy_df=pd.DataFrame({'hpp':hpp,'hqq':hqq,'Fp':Fp,'Fq':Fq,'occp':occp,'occq':occq,'SCFFp':SCFFp,'SCFFq':SCFFq,'SCFOCCp':SCFOCCp,'SCFOCCq':SCFOCCq},index=pair_labels)\n",
    "    # print(f'{k} {time()-t0}')\n",
    "    return pd.concat([rs,dummy_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_two_el(paths,virttt,occcc,froz,Basis_Indices,gen_F,inner_slice,outer_slice,set_indices,set_i_indices,set_j_indices,set_k_indices,set_l_indices):\n",
    "    return dict([(k,gen_two_ints().gen_feat(k,virttt,occcc,froz,Basis_Indices,gen_F,inner_slice,outer_slice,set_indices,set_i_indices,set_j_indices,set_k_indices,set_l_indices))for k in paths])\n",
    "\n",
    "\n",
    "def gen_one_diag(paths,int1,set_indices,gen_F,gen_occ,gen_F_SCF,gen_occ_SCF,pair_labels):\n",
    "    return dict([(k,gen_1_feats(k,int1,set_indices,gen_F,gen_occ,gen_F_SCF,gen_occ_SCF,pair_labels)) for k in paths])\n",
    "\n",
    "\n",
    "def gen_bin(paths,full_set):\n",
    "    # FSO=From same orbital\n",
    "    FSO=[]\n",
    "    # Indexing\n",
    "    featind=[]\n",
    "    # Keys=paths in string format\n",
    "    keys=[]\n",
    "    for i in paths:\n",
    "        k=str(i)\n",
    "        keys.append(k)\n",
    "        for ind,g in enumerate(full_set):\n",
    "# Epq Ers\n",
    "# # e_q+e_s-e_p-e_r\n",
    "# TIUV\n",
    "#'A'+g[0]+'_I'+g[1]+''+g[2]+''+g[3]\n",
    "# Eti Euv (E01 E23)\n",
    "# e_i+e_v-e_u-e_t\n",
    "# e[0] + e[3] - e[1] - e[2]\n",
    "            featind.append(g)\n",
    "            idx=g.split('_')\n",
    "            q=idx[0]\n",
    "            s=idx[1]\n",
    "# Since I!=A just append 0 since they'll never both come from the same orbital                \n",
    "            if q==s:\n",
    "                FSO.append(1)\n",
    "            else:\n",
    "                FSO.append(0)\n",
    "    return dict([(z,pd.DataFrame({'From_Same_Orbital':np.array(FSO).reshape(len(paths),-1)[idx]},index=np.array(featind).reshape(len(paths),-1)[idx])) for idx,z in enumerate(keys)])               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Big_Data_GS(name,paths,full_set,virttt,occcc,froz,Basis_Indices,gen_F,gen_occ,gen_F_SCF,gen_occ_SCF,pair_labels,inner_slice,outer_slice,set_indices,set_i_indices,set_j_indices,set_k_indices,set_l_indices,int1,stacked_pairs):\n",
    "    t0=time()\n",
    "    with open(f'{name}_chain/fixed_feats.pickle', 'wb') as handle:\n",
    "        pickle.dump(pd.concat([pd.concat({k: v for k,v in gen_bin(paths,full_set).items()},axis=0),\n",
    "                               pd.concat({k: v for k,v in gen_two_el(paths,virttt,occcc,froz,Basis_Indices,gen_F,inner_slice,outer_slice,set_indices,set_i_indices,set_j_indices,set_k_indices,set_l_indices).items()},axis=0),\n",
    "                               pd.concat({k: v for k,v in gen_one_diag(paths,int1,set_indices,gen_F,gen_occ,gen_F_SCF,gen_occ_SCF,pair_labels).items()},axis=0)],axis=1), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(f'{name}_chain/fixed_targets.pickle', 'wb') as handle:\n",
    "        pickle.dump(stacked_pairs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in chains:\n",
    "    name = f\"H{i}\"\n",
    "    gen_all(f\"H{i}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DDGNN",
   "language": "python",
   "name": "ddgnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
