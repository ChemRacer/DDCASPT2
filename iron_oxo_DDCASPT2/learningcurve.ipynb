{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data generation\n",
    "import sys\n",
    "# !{sys.executable} -m pip install matplotlib --upgrade\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "#random\n",
    "from time import perf_counter\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score,root_mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Plotting\n",
    "import seaborn as sns\n",
    "sns.set_style()\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.dpi'] = 200\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=np.DeprecationWarning) \n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "# from tqdm.notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e8569-2d81-4deb-b91c-324e49b2d9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "datanums = sum([len(joblib.load(i)) for i in glob('cluster/*pickle')])\n",
    "pairedcp=sns.color_palette('Paired')[8:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_lc(trainsize,testsize):\n",
    "    with open(os.path.join(os.path.expanduser('~'),'DDCASPT2/drop.txt'),'r') as d:\n",
    "        dropfeat = [i.replace('\\n','') for i in d.readlines()]\n",
    "        \n",
    "    radius_range_dirs = []\n",
    "    for i in glob('cluster/*.*'):\n",
    "        try:\n",
    "            radstr = float(os.path.basename(i))\n",
    "            # if radstr>=1.4:\n",
    "            radius_range_dirs.append(os.path.basename(i))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    radius_range = sorted(radius_range_dirs)\n",
    "    \n",
    "    train_ind,test_ind=radius_range[0::2],radius_range[1::2]\n",
    "    \n",
    "    train_ind = list(map(float,train_ind))\n",
    "    test_ind = list(map(float,test_ind))\n",
    "    train_ind, test_ind = train_test_split(train_ind+test_ind,train_size=trainsize, test_size=testsize, random_state=42)\n",
    "    \n",
    "    print(len(train_ind),len(test_ind))\n",
    "    \n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    recover_train = []\n",
    "    recover_test = []\n",
    "    \n",
    "    traincnt = 0\n",
    "    testcnt = 0\n",
    "    \n",
    "    \n",
    "    for k in train_ind:\n",
    "        rad = f\"{k:.2f}\"\n",
    "        traindf = pd.read_csv(os.path.join('cluster',rad,f'{rad}.csv'),compression='zip',index_col=0)\n",
    "        # traindf = traindf[traindf['Pair_Energies'].abs()>=1e-7]\n",
    "        train.append(traindf)\n",
    "        recover_train.append((rad,traincnt,len(traindf)))\n",
    "        traincnt+=1\n",
    "            \n",
    "    for l in test_ind:\n",
    "        rad = f\"{l:.2f}\"\n",
    "        testdf = pd.read_csv(os.path.join('cluster',rad,f'{rad}.csv'),compression='zip',index_col=0)\n",
    "        # testdf = testdf[testdf['Pair_Energies'].abs()>=1e-7]\n",
    "        test.append(testdf)                    \n",
    "        recover_test.append((rad,testcnt,len(testdf)))\n",
    "        testcnt+=1\n",
    "    \n",
    "# rename = {'h$_{qq}^{0}$':'h$_{q}$',\n",
    "# '$(F_{q})_{0}$':'$F_{q}$',\n",
    "# '$(F_{q}^{\\\\text{SCF}})_{0}$':'$F_{q}^{\\\\text{SCF}}$',\n",
    "# '$(\\\\eta_{q})_{0}$':'$\\\\eta_{q}$',\n",
    "# '$(\\\\omega_{q})_{0}$':'$\\\\omega_{q}$',\n",
    "# '$(\\\\eta_{s})_{0}$':'$\\\\eta_{s}$',\n",
    "# 'h$_{ss}^{0}$':'h$_{s}$',\n",
    "# '$(F_{s}^{\\\\text{SCF}})_{0}$':'$F_{s}^{\\\\text{SCF}}$',\n",
    "# '$(F_{s})_{0}$':'$F_{s}$',\n",
    "# '$(\\\\omega_{s})_{0}$':'$\\\\omega_{s}$',\n",
    "# '$(\\\\langle ss \\\\vert ss \\\\rangle)_{0}$':\"$\\\\langle ss \\\\vert ss \\\\rangle$\",\n",
    "# '$(\\\\langle qq \\\\vert qq \\\\rangle)_{0}$':\"$\\langle qq \\\\vert qq \\\\rangle$\",\n",
    "# 'h$_{pp}^{0}$': '(h$_{p}$)$_{0}$',\n",
    "# 'h$_{pp}^{1}$': '(h$_{p}$)$_{1}$',\n",
    "# 'h$_{pp}^{2}$': '(h$_{p}$)$_{2}$',\n",
    "# 'h$_{pp}^{3}$': '(h$_{p}$)$_{3}$',\n",
    "# 'h$_{rr}^{0}$': '(h$_{r}$)$_{0}$',\n",
    "# 'h$_{rr}^{1}$': '(h$_{r}$)$_{1}$',\n",
    "# 'h$_{rr}^{2}$': '(h$_{r}$)$_{2}$',\n",
    "# 'h$_{rr}^{3}$': '(h$_{r}$)$_{3}$',\n",
    "# 'h$_{pq}^{0}$':'(h$_{pq}$)$_{0}$',\n",
    "# 'h$_{pq}^{1}$':'(h$_{pq}$)$_{1}$',\n",
    "# 'h$_{pq}^{2}$':'(h$_{pq}$)$_{2}$',\n",
    "# 'h$_{pq}^{3}$':'(h$_{pq}$)$_{3}$',\n",
    "# 'h$_{pr}^{0}$':'(h$_{pr}$)$_{0}$',\n",
    "# 'h$_{pr}^{1}$':'(h$_{pr}$)$_{1}$',\n",
    "# 'h$_{pr}^{2}$':'(h$_{pr}$)$_{2}$',\n",
    "# 'h$_{pr}^{3}$':'(h$_{pr}$)$_{3}$',\n",
    "# 'h$_{rs}^{0}$':'(h$_{rs}$)$_{0}$',\n",
    "# 'h$_{rs}^{1}$':'(h$_{rs}$)$_{1}$',\n",
    "# 'h$_{rs}^{2}$':'(h$_{rs}$)$_{2}$',\n",
    "# 'h$_{rs}^{3}$':'(h$_{rs}$)$_{3}$',\n",
    "# 'typ_0':'type_0',\n",
    "# 'typ_1':'type_1',\n",
    "# 'typ_2':'type_2',\n",
    "# 'typ_3':'type_3',\n",
    "# '1':\"$\\mathbf{b}$\"         \n",
    "# }\n",
    "\n",
    "\n",
    "    rename = {'h$_{qq}^{0}$':'h$_{q}$',\n",
    "    '$(F_{q})_{0}$':'$F_{q}$',\n",
    "    '$(F_{q}^{\\\\text{SCF}})_{0}$':'$F_{q}^{\\\\text{SCF}}$',\n",
    "    '$(\\\\eta_{q})_{0}$':'$\\\\eta_{q}$',\n",
    "    '$(\\\\omega_{q})_{0}$':'$\\\\omega_{q}$',\n",
    "    '$(\\\\eta_{s})_{0}$':'$\\\\eta_{s}$',\n",
    "    'h$_{ss}^{0}$':'h$_{s}$',\n",
    "    '$(F_{s}^{\\\\text{SCF}})_{0}$':'$F_{s}^{\\\\text{SCF}}$',\n",
    "    '$(F_{s})_{0}$':'$F_{s}$',\n",
    "    '$(\\\\omega_{s})_{0}$':'$\\\\omega_{s}$',\n",
    "    '$(\\\\langle ss \\\\vert ss \\\\rangle)_{0}$':\"$\\\\langle ss \\\\vert ss \\\\rangle$\",\n",
    "    '$(\\\\langle qq \\\\vert qq \\\\rangle)_{0}$':\"$\\langle qq \\\\vert qq \\\\rangle$\",\n",
    "    'h$_{pp}^{0}$': '(h$_{p}$)$_{0}$',\n",
    "    'h$_{pp}^{1}$': '(h$_{p}$)$_{1}$',\n",
    "    'h$_{pp}^{2}$': '(h$_{p}$)$_{2}$',\n",
    "    'h$_{pp}^{3}$': '(h$_{p}$)$_{3}$',\n",
    "    'h$_{rr}^{0}$': '(h$_{r}$)$_{0}$',\n",
    "    'h$_{rr}^{1}$': '(h$_{r}$)$_{1}$',\n",
    "    'h$_{rr}^{2}$': '(h$_{r}$)$_{2}$',\n",
    "    'h$_{rr}^{3}$': '(h$_{r}$)$_{3}$',\n",
    "    'h$_{pq}^{0}$':'(h$_{pq}$)$_{0}$',\n",
    "    'h$_{pq}^{1}$':'(h$_{pq}$)$_{1}$',\n",
    "    'h$_{pq}^{2}$':'(h$_{pq}$)$_{2}$',\n",
    "    'h$_{pq}^{3}$':'(h$_{pq}$)$_{3}$',\n",
    "    'h$_{pr}^{0}$':'(h$_{pr}$)$_{0}$',\n",
    "    'h$_{pr}^{1}$':'(h$_{pr}$)$_{1}$',\n",
    "    'h$_{pr}^{2}$':'(h$_{pr}$)$_{2}$',\n",
    "    'h$_{pr}^{3}$':'(h$_{pr}$)$_{3}$',\n",
    "    'h$_{rs}^{0}$':'(h$_{rs}$)$_{0}$',\n",
    "    'h$_{rs}^{1}$':'(h$_{rs}$)$_{1}$',\n",
    "    'h$_{rs}^{2}$':'(h$_{rs}$)$_{2}$',\n",
    "    'h$_{rs}^{3}$':'(h$_{rs}$)$_{3}$',\n",
    "    'typ_0':'$type_0$',\n",
    "    'typ_1':'$type_1$',\n",
    "    'typ_2':'$type_2$',\n",
    "    'typ_3':'$type_3$',\n",
    "    '1':\"$\\mathbf{b}$\"}\n",
    "        \n",
    "    train_df = pd.concat(train).drop(columns=dropfeat).rename(columns=rename)\n",
    "    test_df = pd.concat(test).drop(columns=dropfeat).rename(columns=rename)\n",
    "    \n",
    "    X_train = train_df.drop(columns=['Pair_Energies']).values\n",
    "    X_test = test_df.drop(columns=['Pair_Energies']).values\n",
    "    \n",
    "    y_train = train_df['Pair_Energies'].values\n",
    "    y_test = test_df['Pair_Energies'].values\n",
    "    \n",
    "    scaler=MinMaxScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    kwargs = dict(pd.read_excel(\"iron_VDZP_params.xlsx\").values)\n",
    "    model=XGBRegressor(**kwargs)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train=model.predict(X_train)\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    print(f\"R2: {r2_score(y_train,y_pred_train):.4f},{r2_score(y_test,y_pred_test):.4f}\")\n",
    "    print(f\"RMSE (mEh): {root_mean_squared_error(y_train,y_pred_train)*1e3:.4f},{root_mean_squared_error(y_test,y_pred_test)*1e3:.4f}\")\n",
    "    \n",
    "    testcntrcvr = 0\n",
    "    recover_test_list = []\n",
    "    recover_test_E2 = []\n",
    "    for j,k,l in recover_test:\n",
    "        init_test = testcntrcvr\n",
    "        testcntrcvr+=l\n",
    "        recover_test_list.append((j,y_test[init_test:testcntrcvr],y_pred_test[init_test:testcntrcvr]))\n",
    "        recover_test_E2.append((j,np.sum(y_test[init_test:testcntrcvr]),np.sum(y_pred_test[init_test:testcntrcvr])))\n",
    "    \n",
    "    \n",
    "    traincntrcvr = 0\n",
    "    recover_train_list = []\n",
    "    recover_train_E2 = []\n",
    "    for j,k,l in recover_train:\n",
    "        init_train = traincntrcvr\n",
    "        traincntrcvr+=l\n",
    "        recover_train_list.append((j,y_train[init_train:traincntrcvr],y_pred_train[init_train:traincntrcvr]))\n",
    "        recover_train_E2.append((j,np.sum(y_train[init_train:traincntrcvr]),np.sum(y_pred_train[init_train:traincntrcvr])))\n",
    "    \n",
    "    traincorrE2 = pd.DataFrame(recover_train_E2).rename(columns={0:'Radius ($\\AA$)',1:'E$_{2}^{\\text{Calculated}}$',2:'E$_{2}^{\\text{Predicted}}$'}).set_index('Radius ($\\AA$)')\n",
    "    #.astype({\"basis set\":str,'Radius ($\\AA$)':float,'E$_{2}^{\\text{Calculated}}$':float,'E$_{2}^{\\text{Predicted}}$':float})\n",
    "    testcorrE2 = pd.DataFrame(recover_test_E2).rename(columns={0:'Radius ($\\AA$)',1:'E$_{2}^{\\text{Calculated}}$',2:'E$_{2}^{\\text{Predicted}}$'}).set_index('Radius ($\\AA$)')\n",
    "    #.astype({\"basis set\":str,'Radius ($\\AA$)':float,'E$_{2}^{\\text{Calculated}}$':float,'E$_{2}^{\\text{Predicted}}$':float})\n",
    "    \n",
    "    \n",
    "    \n",
    "    traincorrE2['CASPT2_E']=np.zeros(len(traincorrE2))\n",
    "    traincorrE2['CASSCF_E']=np.zeros(len(traincorrE2))\n",
    "    traincorrE2['E2']=np.zeros(len(traincorrE2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in traincorrE2.index:\n",
    "        energy_df = pd.read_excel(f\"cluster/{i}/{i}_energies.xlsx\",index_col=0)\n",
    "        traincorrE2.loc[i,'CASPT2_E']=energy_df.loc['CASPT2_E'].values\n",
    "        traincorrE2.loc[i,'CASSCF_E']=energy_df.loc['CASSCF_E'].values\n",
    "        # traincorrE2.loc[i,'E2']=energy_df.loc['E2'].values\n",
    "    \n",
    "    traincorrE2['E$_{\\text{CASPT2}}^{\\text{Predicted}}$'] = traincorrE2['CASSCF_E']+traincorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "    \n",
    "    traincorrE2.rename(columns = {'CASPT2_E':'E$_{\\text{CASPT2}}^{\\text{Calculated}}$','CASSCF_E':'E$_{\\text{CASSCF}}^{\\text{Calculated}}$'},inplace=True)\n",
    "    \n",
    "    \n",
    "    testcorrE2['CASPT2_E']=np.zeros(len(testcorrE2))\n",
    "    testcorrE2['CASSCF_E']=np.zeros(len(testcorrE2))\n",
    "    testcorrE2['E2']=np.zeros(len(testcorrE2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in testcorrE2.index:\n",
    "        energy_df = pd.read_excel(f\"cluster/{i}/{i}_energies.xlsx\",index_col=0)\n",
    "        testcorrE2.loc[i,'CASPT2_E']=energy_df.loc['CASPT2_E'].values\n",
    "        testcorrE2.loc[i,'CASSCF_E']=energy_df.loc['CASSCF_E'].values\n",
    "        # testcorrE2.loc[i,'E2']=energy_df.loc['E2'].values\n",
    "    \n",
    "    testcorrE2['E$_{\\text{CASPT2}}^{\\text{Predicted}}$'] = testcorrE2['CASSCF_E']+testcorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "    \n",
    "    testcorrE2.rename(columns = {'CASPT2_E':'E$_{\\text{CASPT2}}^{\\text{Calculated}}$','CASSCF_E':'E$_{\\text{CASSCF}}^{\\text{Calculated}}$'},inplace=True)\n",
    "    \n",
    "    testcorrE2.reset_index(inplace=True)\n",
    "    traincorrE2.reset_index(inplace=True)    \n",
    "    \n",
    "    return ((y_train,y_pred_train), (y_test,y_pred_test), (traincorrE2, testcorrE2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458b37b-f630-4545-ae5e-b54d2306e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(data,trainsize,save=False):\n",
    "    pair_r2 = []\n",
    "    pair_mae = []\n",
    "    corr_r2 = []\n",
    "    corr_mae = []\n",
    "    for (i,((y_train,y_pred_train), (y_test,y_pred_test), (traincorrE2, testcorrE2))) in data:\n",
    "        traincorrE2_true = traincorrE2['E$_{2}^{\\text{Calculated}}$']\n",
    "        traincorrE2_pred = traincorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "    \n",
    "        testcorrE2_true = testcorrE2['E$_{2}^{\\text{Calculated}}$']\n",
    "        testcorrE2_pred = testcorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "        \n",
    "        pair_r2.append((i,r2_score(y_train,y_pred_train),r2_score(y_test,y_pred_test)))\n",
    "        pair_mae.append((i,1e3 * mean_absolute_error(y_train,y_pred_train),1e3 * mean_absolute_error(y_test,y_pred_test)))\n",
    "    \n",
    "        corr_r2.append((i,r2_score(traincorrE2_true,traincorrE2_pred),r2_score(testcorrE2_true,testcorrE2_pred)))\n",
    "        corr_mae.append((i,1e3 * mean_absolute_error(traincorrE2_true,traincorrE2_pred),1e3 * mean_absolute_error(testcorrE2_true,testcorrE2_pred)))\n",
    "    \n",
    "    dfpair_r2 = pd.DataFrame(pair_r2,columns=['Train Size', \"Train\", \"Test\"])\n",
    "    dfcorr_r2 = pd.DataFrame(corr_r2,columns=['Train Size', \"Train\", \"Test\"])\n",
    "    dfpair_mae = pd.DataFrame(pair_mae,columns=['Train Size', \"Train\", \"Test\"])\n",
    "    dfcorr_mae = pd.DataFrame(corr_mae,columns=['Train Size', \"Train\", \"Test\"])\n",
    "    # dfcorr_r2.to_excel('corrr2.xlsx')\n",
    "    print(dfcorr_r2)\n",
    "    fig, ax = plt.subplots(2,2,sharex=True)\n",
    "    sns.lineplot(dfpair_r2.melt(id_vars='Train Size',value_vars=['Train','Test']),x='Train Size',y='value', hue='variable',ax=ax[0][0],palette=pairedcp)\n",
    "    sns.lineplot(dfcorr_r2.melt(id_vars='Train Size',value_vars=['Train','Test']),x='Train Size',y='value', hue='variable',ax=ax[0][1],markers='x',palette=pairedcp)\n",
    "    ax[0][0].set_title(r'Pair-Energies ($\\varepsilon_{qs}$)')\n",
    "    ax[0][1].set_title(r'Correlation Energies (E$_{2}$)')    \n",
    "    ax[0][0].set_xlim(0,700)\n",
    "    ax[0][1].set_xlim(0,700)\n",
    "    ax[0][0].set_ylim(-.74,1.1)\n",
    "    ax[0][1].set_ylim(-.74,1.1)\n",
    "    ax[0][0].set_ylabel(\"R$^{2}$\")\n",
    "    ax[0][1].set_ylabel(\"R$^{2}$\")\n",
    "    sns.lineplot(dfpair_mae.melt(id_vars='Train Size',value_vars=['Train','Test']),x='Train Size',y='value', hue='variable',ax=ax[1][0],palette=pairedcp)\n",
    "    ax[1][0].set_ylim(0,0.25)\n",
    "    sns.lineplot(dfcorr_mae.melt(id_vars='Train Size',value_vars=['Train','Test']),x='Train Size',y='value', hue='variable',ax=ax[1][1],palette=pairedcp)\n",
    "    ax[1][1].set_ylim(0,8)\n",
    "    ax[1][0].set_ylabel(\"MAE (mE$_{h}$)\")\n",
    "    ax[1][1].set_ylabel(\"MAE (mE$_{h}$)\")\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig('images/learning_curves.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb76a34-c8ce-4b63-bf98-948f628ceaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    ".1 * 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b394ae-edfe-4011-874e-522e0fe7f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,0.9]\n",
    "\n",
    "data = []\n",
    "for i in ranges:\n",
    "    # print(i,j)\n",
    "    data.append((int(i * datanums),gen_lc(i,0.1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2dfca-5317-4fc6-895c-cf5dd9e326ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data,0.1,True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187abe9-b429-4d49-9f05-664d05833a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ranges=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,0.9]\n",
    "# for j in ranges:\n",
    "#     data = []\n",
    "#     for i in ranges:\n",
    "#         if j+i<=1:\n",
    "#             # print(i,j)\n",
    "#             data.append((int(i * datanums),gen_lc(i,j)))\n",
    "#     plot(data,j)\n",
    "#     plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0a8ff-aff6-4cf0-9126-f995098dd7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_cut(cutoff):\n",
    "    with open(os.path.join(os.path.expanduser('~'),'DDCASPT2/drop.txt'),'r') as d:\n",
    "        dropfeat = [i.replace('\\n','') for i in d.readlines()]\n",
    "        \n",
    "    radius_range_dirs = []\n",
    "    for i in glob('cluster/*.*'):\n",
    "        try:\n",
    "            radstr = float(os.path.basename(i))\n",
    "            # if radstr>=1.4:\n",
    "            radius_range_dirs.append(os.path.basename(i))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    radius_range = sorted(radius_range_dirs)\n",
    "    \n",
    "    train_ind,test_ind=radius_range[0::2],radius_range[1::2]\n",
    "    \n",
    "    train_ind = list(map(float,train_ind))\n",
    "    test_ind = list(map(float,test_ind))\n",
    "    train_ind, test_ind = train_test_split(train_ind+test_ind, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(len(train_ind),len(test_ind))\n",
    "    \n",
    "    \n",
    "    train = []\n",
    "    test = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    recover_train = []\n",
    "    recover_test = []\n",
    "    \n",
    "    traincnt = 0\n",
    "    testcnt = 0\n",
    "    \n",
    "    \n",
    "    for k in train_ind:\n",
    "        rad = f\"{k:.2f}\"\n",
    "        traindf = pd.read_csv(os.path.join('cluster',rad,f'{rad}.csv'),compression='zip',index_col=0)\n",
    "        traindf = traindf[traindf['Pair_Energies'].abs()>=cutoff]\n",
    "        train.append(traindf)\n",
    "        recover_train.append((rad,traincnt,len(traindf)))\n",
    "        traincnt+=1\n",
    "            \n",
    "    for l in test_ind:\n",
    "        rad = f\"{l:.2f}\"\n",
    "        testdf = pd.read_csv(os.path.join('cluster',rad,f'{rad}.csv'),compression='zip',index_col=0)\n",
    "        testdf = testdf[testdf['Pair_Energies'].abs()>=cutoff]\n",
    "        test.append(testdf)                    \n",
    "        recover_test.append((rad,testcnt,len(testdf)))\n",
    "        testcnt+=1\n",
    "    \n",
    "    rename = {'h$_{qq}^{0}$':'h$_{q}$',\n",
    "    '$(F_{q})_{0}$':'$F_{q}$',\n",
    "    '$(F_{q}^{\\\\text{SCF}})_{0}$':'$F_{q}^{\\\\text{SCF}}$',\n",
    "    '$(\\\\eta_{q})_{0}$':'$\\\\eta_{q}$',\n",
    "    '$(\\\\omega_{q})_{0}$':'$\\\\omega_{q}$',\n",
    "    '$(\\\\eta_{s})_{0}$':'$\\\\eta_{s}$',\n",
    "    'h$_{ss}^{0}$':'h$_{s}$',\n",
    "    '$(F_{s}^{\\\\text{SCF}})_{0}$':'$F_{s}^{\\\\text{SCF}}$',\n",
    "    '$(F_{s})_{0}$':'$F_{s}$',\n",
    "    '$(\\\\omega_{s})_{0}$':'$\\\\omega_{s}$',\n",
    "    '$(\\\\langle ss \\\\vert ss \\\\rangle)_{0}$':\"$\\\\langle ss \\\\vert ss \\\\rangle$\",\n",
    "    '$(\\\\langle qq \\\\vert qq \\\\rangle)_{0}$':\"$\\langle qq \\\\vert qq \\\\rangle$\",\n",
    "    'h$_{pp}^{0}$': 'h$_{p}^{0}$',\n",
    "    'h$_{pp}^{1}$': 'h$_{p}^{1}$',\n",
    "    'h$_{pp}^{2}$': 'h$_{p}^{2}$',\n",
    "    'h$_{pp}^{3}$': 'h$_{p}^{3}$',\n",
    "    'h$_{rr}^{0}$': 'h$_{r}^{0}$',\n",
    "    'h$_{rr}^{1}$': 'h$_{r}^{1}$',\n",
    "    'h$_{rr}^{2}$': 'h$_{r}^{2}$',\n",
    "    'h$_{rr}^{3}$': 'h$_{r}^{3}$'}\n",
    "    \n",
    "    train_df = pd.concat(train).drop(columns=dropfeat).rename(columns=rename)\n",
    "    test_df = pd.concat(test).drop(columns=dropfeat).rename(columns=rename)\n",
    "    \n",
    "    X_train = train_df.drop(columns=['Pair_Energies']).values\n",
    "    X_test = test_df.drop(columns=['Pair_Energies']).values\n",
    "    \n",
    "    y_train = train_df['Pair_Energies'].values\n",
    "    y_test = test_df['Pair_Energies'].values\n",
    "    \n",
    "    scaler=MinMaxScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    kwargs = dict(pd.read_excel(\"iron_VDZP_params.xlsx\").values)\n",
    "    model=XGBRegressor(**kwargs)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred_train=model.predict(X_train)\n",
    "    y_pred_test=model.predict(X_test)\n",
    "    print(f\"R2: {r2_score(y_train,y_pred_train):.4f},{r2_score(y_test,y_pred_test):.4f}\")\n",
    "    print(f\"RMSE (mEh): {root_mean_squared_error(y_train,y_pred_train)*1e3:.4f},{root_mean_squared_error(y_test,y_pred_test)*1e3:.4f}\")\n",
    "    \n",
    "    testcntrcvr = 0\n",
    "    recover_test_list = []\n",
    "    recover_test_E2 = []\n",
    "    for j,k,l in recover_test:\n",
    "        init_test = testcntrcvr\n",
    "        testcntrcvr+=l\n",
    "        recover_test_list.append((j,y_test[init_test:testcntrcvr],y_pred_test[init_test:testcntrcvr]))\n",
    "        recover_test_E2.append((j,np.sum(y_test[init_test:testcntrcvr]),np.sum(y_pred_test[init_test:testcntrcvr])))\n",
    "    \n",
    "    \n",
    "    traincntrcvr = 0\n",
    "    recover_train_list = []\n",
    "    recover_train_E2 = []\n",
    "    for j,k,l in recover_train:\n",
    "        init_train = traincntrcvr\n",
    "        traincntrcvr+=l\n",
    "        recover_train_list.append((j,y_train[init_train:traincntrcvr],y_pred_train[init_train:traincntrcvr]))\n",
    "        recover_train_E2.append((j,np.sum(y_train[init_train:traincntrcvr]),np.sum(y_pred_train[init_train:traincntrcvr])))\n",
    "    \n",
    "    traincorrE2 = pd.DataFrame(recover_train_E2).rename(columns={0:'Radius ($\\AA$)',1:'E$_{2}^{\\text{Calculated}}$',2:'E$_{2}^{\\text{Predicted}}$'}).set_index('Radius ($\\AA$)')\n",
    "    #.astype({\"basis set\":str,'Radius ($\\AA$)':float,'E$_{2}^{\\text{Calculated}}$':float,'E$_{2}^{\\text{Predicted}}$':float})\n",
    "    testcorrE2 = pd.DataFrame(recover_test_E2).rename(columns={0:'Radius ($\\AA$)',1:'E$_{2}^{\\text{Calculated}}$',2:'E$_{2}^{\\text{Predicted}}$'}).set_index('Radius ($\\AA$)')\n",
    "    #.astype({\"basis set\":str,'Radius ($\\AA$)':float,'E$_{2}^{\\text{Calculated}}$':float,'E$_{2}^{\\text{Predicted}}$':float})\n",
    "    \n",
    "    \n",
    "    \n",
    "    traincorrE2['CASPT2_E']=np.zeros(len(traincorrE2))\n",
    "    traincorrE2['CASSCF_E']=np.zeros(len(traincorrE2))\n",
    "    traincorrE2['E2']=np.zeros(len(traincorrE2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in traincorrE2.index:\n",
    "        energy_df = pd.read_excel(f\"cluster/{i}/{i}_energies.xlsx\",index_col=0)\n",
    "        traincorrE2.loc[i,'CASPT2_E']=energy_df.loc['CASPT2_E'].values\n",
    "        traincorrE2.loc[i,'CASSCF_E']=energy_df.loc['CASSCF_E'].values\n",
    "        # traincorrE2.loc[i,'E2']=energy_df.loc['E2'].values\n",
    "    \n",
    "    traincorrE2['E$_{\\text{CASPT2}}^{\\text{Predicted}}$'] = traincorrE2['CASSCF_E']+traincorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "    \n",
    "    traincorrE2.rename(columns = {'CASPT2_E':'E$_{\\text{CASPT2}}^{\\text{Calculated}}$','CASSCF_E':'E$_{\\text{CASSCF}}^{\\text{Calculated}}$'},inplace=True)\n",
    "    \n",
    "    \n",
    "    testcorrE2['CASPT2_E']=np.zeros(len(testcorrE2))\n",
    "    testcorrE2['CASSCF_E']=np.zeros(len(testcorrE2))\n",
    "    testcorrE2['E2']=np.zeros(len(testcorrE2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in testcorrE2.index:\n",
    "        energy_df = pd.read_excel(f\"cluster/{i}/{i}_energies.xlsx\",index_col=0)\n",
    "        testcorrE2.loc[i,'CASPT2_E']=energy_df.loc['CASPT2_E'].values\n",
    "        testcorrE2.loc[i,'CASSCF_E']=energy_df.loc['CASSCF_E'].values\n",
    "        # testcorrE2.loc[i,'E2']=energy_df.loc['E2'].values\n",
    "    \n",
    "    testcorrE2['E$_{\\text{CASPT2}}^{\\text{Predicted}}$'] = testcorrE2['CASSCF_E']+testcorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "    \n",
    "    testcorrE2.rename(columns = {'CASPT2_E':'E$_{\\text{CASPT2}}^{\\text{Calculated}}$','CASSCF_E':'E$_{\\text{CASSCF}}^{\\text{Calculated}}$'},inplace=True)\n",
    "    \n",
    "    testcorrE2.reset_index(inplace=True)\n",
    "    traincorrE2.reset_index(inplace=True)    \n",
    "    \n",
    "    return ((y_train,y_pred_train), (y_test,y_pred_test), (traincorrE2, testcorrE2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc70c0dd-7a42-41b7-8499-95a85efb9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "14-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b973a0-ff52-4b82-bdcf-1d1a4d1af96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076bf53e-fe7b-4aae-921f-eb6d8c84c9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in np.logspace(-14,-3,12):\n",
    "    print(i)\n",
    "    data.append((i,gen_cut(i)))\n",
    "\n",
    "\n",
    "pair_r2 = []\n",
    "pair_mae = []\n",
    "corr_r2 = []\n",
    "corr_mae = []\n",
    "for (i,((y_train,y_pred_train), (y_test,y_pred_test), (traincorrE2, testcorrE2))) in data:\n",
    "    traincorrE2_true = traincorrE2['E$_{2}^{\\text{Calculated}}$']\n",
    "    traincorrE2_pred = traincorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "\n",
    "    testcorrE2_true = testcorrE2['E$_{2}^{\\text{Calculated}}$']\n",
    "    testcorrE2_pred = testcorrE2['E$_{2}^{\\text{Predicted}}$']\n",
    "    \n",
    "    pair_r2.append((i,r2_score(y_train,y_pred_train),r2_score(y_test,y_pred_test)))\n",
    "    pair_mae.append((i,1e3 * mean_absolute_error(y_train,y_pred_train),1e3 * mean_absolute_error(y_test,y_pred_test)))\n",
    "\n",
    "    corr_r2.append((i,r2_score(traincorrE2_true,traincorrE2_pred),r2_score(testcorrE2_true,testcorrE2_pred)))\n",
    "    corr_mae.append((i,1e3 * mean_absolute_error(traincorrE2_true,traincorrE2_pred),1e3 * mean_absolute_error(testcorrE2_true,testcorrE2_pred)))\n",
    "\n",
    "dfpair_r2 = pd.DataFrame(pair_r2,columns=['Cutoff', \"Train\", \"Test\"])\n",
    "dfcorr_r2 = pd.DataFrame(corr_r2,columns=['Cutoff', \"Train\", \"Test\"])\n",
    "dfpair_mae = pd.DataFrame(pair_mae,columns=['Cutoff', \"Train\", \"Test\"])\n",
    "dfcorr_mae = pd.DataFrame(corr_mae,columns=['Cutoff', \"Train\", \"Test\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b980984b-13aa-4e3f-95c6-9396ba866ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpair_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d90bfe-8878-4ca8-9c2c-92a858aed967",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairedcp=sns.color_palette('Paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9337cab-f17d-40ef-b0c9-a01fcc6df6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c1a701-0eb2-494b-87c1-5b8716082b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2,sharex=True)\n",
    "sns.lineplot(dfpair_r2.melt(id_vars='Cutoff',value_vars=['Train','Test']),x='Cutoff',y='value', hue='variable',ax=ax[0][0],palette=pairedcp[8:10])\n",
    "sns.lineplot(dfcorr_r2.melt(id_vars='Cutoff',value_vars=['Train','Test']),x='Cutoff',y='value', hue='variable',ax=ax[0][1],markers='x',palette=pairedcp[8:10])\n",
    "# ax[0][0].set_xlim(0,700)\n",
    "# ax[0][1].set_xlim(0,700)\n",
    "ax[0][0].set_title(r'Pair-Energies ($\\varepsilon_{qs}$)')\n",
    "ax[0][1].set_title(r'Correlation Energies (E$_{2}$)')\n",
    "ax[0][0].set_xscale(\"log\")\n",
    "ax[0][1].set_xscale(\"log\")\n",
    "ax[0][0].set_ylim(-.74,1.1)\n",
    "ax[0][1].set_ylim(-.74,1.1)\n",
    "ax[0][0].set_xlim(10**-15,0)\n",
    "# ax[0][1].set_xlim(-.74,1.1)\n",
    "ax[0][0].set_ylabel(\"R$^{2}$\")\n",
    "ax[0][1].set_ylabel(\"R$^{2}$\")\n",
    "sns.lineplot(dfpair_mae.melt(id_vars='Cutoff',value_vars=['Train','Test']),x='Cutoff',y='value', hue='variable',ax=ax[1][0],palette=pairedcp[8:10])\n",
    "ax[1][0].set_ylim(0,0.25)\n",
    "sns.lineplot(dfcorr_mae.melt(id_vars='Cutoff',value_vars=['Train','Test']),x='Cutoff',y='value', hue='variable',ax=ax[1][1],palette=pairedcp[8:10])\n",
    "ax[1][1].set_ylim(0,8)\n",
    "ax[1][0].set_ylabel(\"MAE (mE$_{h}$)\")\n",
    "ax[1][1].set_ylabel(\"MAE (mE$_{h}$)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/cutoff_curves.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbcaa6-7111-4200-8dca-d99d31c10abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcorr_r2.sort_values(by=['Train','Test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590267c5-9f02-4653-ad33-53c37862f2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
