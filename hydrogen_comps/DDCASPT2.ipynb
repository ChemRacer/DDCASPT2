{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install --upgrade h5py numpy scipy matplotlib seaborn plumbum\n",
    "#######################################################\n",
    "#Import packages\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from math import sin, cos, pi\n",
    "import glob\n",
    "import subprocess\n",
    "import pickle\n",
    "from subprocess import call, check_output\n",
    "import pandas as pd\n",
    "# import psi4\n",
    "\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from plumbum.cmd import grep, awk\n",
    "\n",
    "import shutil\n",
    "import random\n",
    "import sklearn\n",
    "from shutil import copy\n",
    "import csv\n",
    "import h5py as h5\n",
    "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Obital labels\n",
    "## Inactive i,j\n",
    "## Active t,u,v\n",
    "## Virtual a,b\n",
    "\n",
    "## Type 1: IA->AA\n",
    "## Type 2: II->AA (P)\n",
    "## Type 3: II->AA (M)\n",
    "## Type 4: AA->VA\n",
    "## Type 5: IA->VA/AV\n",
    "## Type 6: II->AV (P)\n",
    "## Type 7: II->AV (M)\n",
    "## Type 8: AA->VV (P)\n",
    "## Type 9: AA->VV (M)\n",
    "## Type 10: IA->VV (P)\n",
    "## Type 11: IA->VV (M)\n",
    "## Type 12: II->VV (P)\n",
    "## Type 13: II->VV (M)\n",
    "\n",
    "## A: IA->AA\n",
    "## B: II->AA\n",
    "## C: AA->VA\n",
    "## D: IA->VA/AV\n",
    "## E: II->AV\n",
    "## F: AA->VV\n",
    "## G: IA->VV \n",
    "## H: II->VV\n",
    "#######################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.arange(106,181,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set precision, we can change this at will\n",
    "np.set_printoptions(precision=4)\n",
    "# pd.set_option('precision', 4)\n",
    "\n",
    "# This tells us the number of structures and how theyre labeled\n",
    "# structs=np.arange(106,181,1)\n",
    "structs=np.arange(106,181,0.5)\n",
    "# structs=np.arange(150,171,20)\n",
    "# CASPT2 excitation types\n",
    "excittype=np.arange(1,14,1)\n",
    "# Define molecule\n",
    "typ='O3'\n",
    "\n",
    "# Find structures\n",
    "struct_name=[i for i in os.listdir() if i.endswith('.xyz') and i.startswith(f'{typ}') and '_' in i]\n",
    "\n",
    "if len(struct_name)==0:\n",
    "    for i in structs:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),'Coords',f'O3_{i}.xyz')):\n",
    "            struct_name.append(f'O3_{i}.xyz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(structs))\n",
    "with open('keys.pickle', 'wb') as handle:\n",
    "    pickle.dump([str(i) for i in structs], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "l=[str(i) for i in structs]\n",
    "l=[i for i in l if float(i)>=110.0 and float(i)<=170.0]\n",
    "print(len(l))\n",
    "# train_ind,test_ind=train_test_split(l, test_size=0.3, random_state=0)\n",
    "# print(len(train_ind),len(test_ind))\n",
    "# with open('train_ind.pickle', 'wb') as handle:\n",
    "#     pickle.dump(train_ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('test_ind.pickle', 'wb') as handle:\n",
    "#     pickle.dump(test_ind, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('test_ind.pickle', 'rb') as handle:\n",
    "    test_ind = pickle.load(handle)\n",
    "\n",
    "with open('train_ind.pickle', 'rb') as handle:\n",
    "    train_ind = pickle.load(handle)\n",
    "    \n",
    "print(len(train_ind),len(test_ind))    \n",
    "structs=l\n",
    "print(len(structs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete excessive extra files\n",
    "def del_useless():\n",
    "    '''\n",
    "    Delete the extra files\n",
    "    '''\n",
    "    for i in os.listdir():\n",
    "        for j in ['status','GssOrb','LprOrb','LoProp','guessorb','xmldump','RasOrb','SpdOrb']:\n",
    "            if j in i:\n",
    "                os.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When restarting a setr of calculations just clear everyting out\n",
    "def clean_dir():\n",
    "    for entry in os.scandir(path=os.getcwd()):\n",
    "        if entry.is_dir():\n",
    "            if entry.name=='Fock':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if entry.name=='hdf5':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if entry.name=='e2':\n",
    "                shutil.rmtree(entry.name)                \n",
    "            if entry.name=='Labels':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if entry.name=='Coords':\n",
    "                shutil.rmtree(entry.name)\n",
    "            if 'dir' in entry.name:\n",
    "                shutil.rmtree(entry.name)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this before clean_dir, this pulls the xyz files out just to \n",
    "def pull_xyz():\n",
    "    import re\n",
    "    for i in struct_name:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),i))==False and os.path.exists(os.path.join(os.getcwd(),'Coords')):\n",
    "            shutil.copy(os.path.join(os.getcwd(),'/'.join(('Coords',i))),os.path.join(os.getcwd(),i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_set='ANO-RCC-VDZP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_gateway(angle):\n",
    "    string=f'''&GATEWAY \n",
    "coord={f'O3_{angle}.xyz'}\n",
    "Basis = {basis_set}\n",
    "Group = nosymm\n",
    "End of Input\n",
    "\n",
    "\n",
    "'''\n",
    "    return string\n",
    "def gen_seward():\n",
    "    string=f'''&SEWARD\n",
    "End of Input\n",
    "'''\n",
    "    return string\n",
    "\n",
    "def gen_motra(angle):\n",
    "    string=f'''&MOTRA\n",
    "Frozen=0\n",
    ">>> COPY $WorkDir/GMJ_one_int_indx.csv $CurrDir/{angle}.GMJ_one_int_indx.csv\n",
    ">>> COPY $WorkDir/GMJ_one_int.csv $CurrDir/{angle}.GMJ_one_int.csv\n",
    ">>> COPY $WorkDir/GMJ_two_int_indx.csv $CurrDir/{angle}.GMJ_two_int_indx.csv\n",
    ">>> COPY $WorkDir/GMJ_two_int.csv $CurrDir/{angle}.GMJ_two_int.csv\n",
    "\n",
    "'''\n",
    "    return string\n",
    "\n",
    "def gen_scf(angle):\n",
    "    string=f\"\"\"&SCF &END\n",
    ">>> COPY $WorkDir/PT2_ozone_{angle}.scf.h5 $CurrDir/\n",
    "\"\"\"\n",
    "    return string    \n",
    "\n",
    "\n",
    "def gen_rasscf(angle):\n",
    "    string=f\"\"\"&RASSCF &END\n",
    "Title= RASSCF\n",
    "fileorb\n",
    "{fileorb}\n",
    "NACTEL\n",
    "4 0 0\n",
    "Inactive\n",
    "10\n",
    "RAS2\n",
    "3\n",
    "Symmetry\n",
    "1\n",
    "Spin\n",
    "1\n",
    "orblisting\n",
    "all\n",
    "ITERation\n",
    "200 100\n",
    "CIMX\n",
    "200\n",
    "SDAV\n",
    "500\n",
    "PRWF\n",
    "0\n",
    "PRSD\n",
    ">>> COPY $WorkDir/PT2_ozone_{angle}.rasscf.h5 $CurrDir/\n",
    ">>> COPY $WorkDir/GMJ_Fock_MO.csv $CurrDir/{angle}.GMJ_Fock_MO.csv\n",
    "\"\"\"\n",
    "    return string    \n",
    "\n",
    "def gen_caspt2():\n",
    "    string=\"\"\"&CASPT2 &END\n",
    "Frozen \n",
    "0\n",
    "Imaginary Shift\n",
    "0.0\n",
    "\n",
    ">>foreach i in (B,E,F,G,H)\n",
    ">>foreach j in (P,M)\n",
    ">>if ( -FILE GMJ_e2_${i}_${j}.csv )\n",
    ">>> COPY $WorkDir/GMJ_RHS_${i}_${j}.csv $CurrDir/GMJ_RHS_${i}_${j}.csv\n",
    ">>> COPY $WorkDir/GMJ_IVECW_${i}_${j}.csv $CurrDir/GMJ_IVECW_${i}_${j}.csv\n",
    ">>> COPY $WorkDir/GMJ_e2_${i}_${j}.csv $CurrDir/GMJ_e2_${i}_${j}.csv\n",
    ">>endif\n",
    ">>enddo\n",
    ">>enddo\n",
    "\n",
    ">>foreach i in (A,C,D)\n",
    ">>if ( -FILE GMJ_e2_$i.csv )\n",
    ">>> COPY $WorkDir/GMJ_RHS_$i.csv $CurrDir/GMJ_RHS_$i.csv\n",
    ">>> COPY $WorkDir/GMJ_IVECW_$i.csv $CurrDir/GMJ_IVECW_$i.csv\n",
    ">>> COPY $WorkDir/GMJ_e2_$i.csv $CurrDir/GMJ_e2_$i.csv\n",
    ">>endif\n",
    ">>enddo\n",
    "\n",
    "\"\"\"\n",
    "    return string    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_XYZ(angle):\n",
    "    f=open(f'O3_{angle}.xyz','w+')\n",
    "#     radius=1.472\n",
    "    radius=1.278\n",
    "    O1=f'O {radius*cos(((angle/2)*(pi/180))):.6f} {radius*sin((angle/2)*(pi/180)):.6f} {0.0000:.6f}'\n",
    "    O2=f'O {0.0000:.6f} {0.0000:.6f} {0.0000:.6f}'\n",
    "    O3=f'O {radius*cos(-(angle/2)*(pi/180)):.6f} {radius*sin(-(angle/2)*(pi/180)):.6f} {0.0000:.6f}'\n",
    "    mylist=[O1, O2, O3]\n",
    "    f.write(str(len(mylist))+'\\n'+'\\n')\n",
    "    [f.write(i+'\\n') for i in mylist]\n",
    "    f.close()\n",
    "    return mylist\n",
    "    \n",
    "\n",
    "def gen_eq_orbs():\n",
    "    angle=118\n",
    "    gen_XYZ(angle)\n",
    "    new_input=f'''&GATEWAY \n",
    "coord={f'O3_{angle}.xyz'}\n",
    "Basis = {basis_set}\n",
    "Group = nosymm\n",
    "End of Input\n",
    "\n",
    "\n",
    "\n",
    "&SEWARD\n",
    "End of Input\n",
    "\n",
    "&RASSCF &END\n",
    "Title= RASSCF\n",
    "NACTEL\n",
    "4 0 0\n",
    "Inactive\n",
    "10\n",
    "RAS2\n",
    "3\n",
    "Symmetry\n",
    "1\n",
    "Spin\n",
    "1\n",
    "ALTEr=2; 1 7 11; 1 10 12;\n",
    "orblisting\n",
    "all\n",
    "ITERation\n",
    "200 100\n",
    "CIMX\n",
    "200\n",
    "SDAV\n",
    "500\n",
    "PRWF\n",
    "0\n",
    "PRSD\n",
    "\n",
    "&CASPT2 &END\n",
    "Frozen\n",
    "0\n",
    "Imaginary Shift\n",
    "0.0\n",
    "'''\n",
    "    g=open(f'eq_orbs.input', 'wb')\n",
    "    g.write(new_input.encode())\n",
    "    g.close()\n",
    "    call(['pymolcas', f'eq_orbs.input', '-oe', f'eq_orbs.output'])\n",
    "\n",
    "# ALTEr=3; 1 10 11; 1 9 12; 1 13 13\n",
    "\n",
    "fileorb=os.path.join(os.getcwd(),'eq_orbs.RasOrb')\n",
    "\n",
    "def Gen_Scan():\n",
    "    import re\n",
    "    for angle in structs:\n",
    "        gen_XYZ(float(angle))\n",
    "        g=open(f'PT2_ozone_{angle}.input', 'wb')\n",
    "        g.write(gen_gateway(angle).encode())\n",
    "        g.write(gen_seward().encode())\n",
    "        g.write(gen_motra(angle).encode())\n",
    "        g.write(gen_scf(angle).encode())        \n",
    "        g.write(gen_rasscf(angle).encode())        \n",
    "        g.write(gen_caspt2().encode())\n",
    "        g.close()\n",
    "        call(['pymolcas','-new','-clean',f'PT2_ozone_{angle}.input', '-oe', f'PT2_ozone_{angle}.output'])\n",
    "        [shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),f'{angle}.{i}')) for i in glob.glob(f'GMJ_RHS_*')]\n",
    "        [shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),f'{angle}.{i}')) for i in glob.glob(f'GMJ_IVECW_*')]        \n",
    "        [shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),f'{angle}.{i}')) for i in glob.glob(f'GMJ_e2_*')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move input and output files to their proper directory\n",
    "def mv_fils():\n",
    "    '''\n",
    "    Move the files to directories named for them\n",
    "    '''\n",
    "    import shutil\n",
    "    import os\n",
    "    for i in structs:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),f'{i}_dir')) is True:\n",
    "            if os.path.exists(os.path.join(os.getcwd(),f'PT2_ozone_{i}.output'))==True:\n",
    "                shutil.move(os.path.join(os.getcwd(),f'PT2_ozone_{i}.output'),os.path.join(os.getcwd(),f'{i}_dir',f'PT2_ozone_{i}.output'))\n",
    "            if os.path.exists(os.path.join(os.getcwd(),f'PT2_ozone_{i}.input'))==True:\n",
    "                shutil.move(os.path.join(os.getcwd(),f'PT2_ozone_{i}.input'),os.path.join(os.getcwd(),f'{i}_dir',f'PT2_ozone_{i}.input'))\n",
    "        else:\n",
    "            os.mkdir(f'{i}_dir')\n",
    "            if os.path.exists(os.path.join(os.getcwd(),f'PT2_ozone_{i}.output'))==True:\n",
    "                shutil.move(os.path.join(os.getcwd(),f'PT2_ozone_{i}.output'),os.path.join(os.getcwd(),f'{i}_dir',f'PT2_ozone_{i}.output'))\n",
    "            if os.path.exists(os.path.join(os.getcwd(),f'PT2_ozone_{i}.input'))==True:\n",
    "                shutil.move(os.path.join(os.getcwd(),f'PT2_ozone_{i}.input'),os.path.join(os.getcwd(),f'{i}_dir',f'PT2_ozone_{i}.input'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data files to directories\n",
    "def mv_dat():\n",
    "# Move h5 and coord files to dirs\n",
    "    for i in os.listdir():\n",
    "        if i.endswith('h5'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'hdf5')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('hdf5',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'hdf5')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('hdf5',i))))\n",
    "# Move xyz files\n",
    "        if i.endswith('.xyz') and i.startswith(f'{typ}') and '_' in i:\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'Coords')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('Coords',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'Coords')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('Coords',i))))\n",
    "# Move pair energy files\n",
    "        if '_e2' in i and i.endswith('csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'e2')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('e2',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'e2')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('e2',i))))\n",
    "# Move rhs files which tell how the ivvecc2 and ivecw files are indexed\n",
    "        if 'RHS' in i and i.endswith('csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'Labels')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('Labels',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'Labels')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('Labels',i))))\n",
    "        if 'IVECW' in i and i.endswith('csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'IVECW')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('IVECW',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'IVECW')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('IVECW',i))))\n",
    "\n",
    "                    \n",
    "# MO fock matrix\n",
    "        if 'Fock' in i and i.endswith('csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'Fock')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('Fock',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'Fock')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('Fock',i))))\n",
    "        if i.endswith('_one_int_indx.csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'1_int_idx')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('1_int_idx',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'1_int_idx')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('1_int_idx',i))))\n",
    "        if i.endswith('_two_int_indx.csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'2_int_idx')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('2_int_idx',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'2_int_idx')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('2_int_idx',i))))\n",
    "        if i.endswith('GMJ_one_int.csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'1_ints')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('1_ints',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'1_ints')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('1_ints',i))))\n",
    "        if i.endswith('GMJ_two_int.csv'):\n",
    "            if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "                if os.path.exists(os.path.join(os.getcwd(),f'2_ints')) is True:\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('2_ints',i))))\n",
    "                else:\n",
    "                    os.mkdir(f'2_ints')\n",
    "                    shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('2_ints',i))))\n",
    "#         if i.endswith('GMJ_AO_INT.csv'):\n",
    "#             if os.path.exists(os.path.join(os.getcwd(),i))==True:\n",
    "#                 if os.path.exists(os.path.join(os.getcwd(),f'AO_ints')) is True:\n",
    "#                     shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('AO_ints',i))))\n",
    "#                 else:\n",
    "#                     os.mkdir(f'AO_ints')\n",
    "#                     shutil.move(os.path.join(os.getcwd(),i),os.path.join(os.getcwd(),'/'.join(('AO_ints',i))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run():\n",
    "    # Pull xyz structures\n",
    "    pull_xyz()\n",
    "    # The rest follows as labeled above\n",
    "#     clean_dir()\n",
    "#     gen_eq_orbs()    \n",
    "    Gen_Scan() \n",
    "    mv_fils()\n",
    "    del_useless()\n",
    "    mv_dat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_useless()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a new run\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab caspt2 energies\n",
    "def caspt2():\n",
    "    PT2=[]\n",
    "    PT2List=[]\n",
    "    PT2labels=[]\n",
    "\n",
    "    for j in structs:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),f'{j}_dir'))==True:\n",
    "            path=os.path.join(os.getcwd(),f'{j}_dir',f'PT2_ozone_{j}.output')\n",
    "            PT2labels.append(j)\n",
    "            PT2.append(float((grep['-i', '::    CASPT2',path] | awk['{print $NF }'])()))\n",
    "    PT2List=np.array(PT2)\n",
    "    PT2Dict=pd.DataFrame({'Label':PT2labels,'PT2':PT2List})\n",
    "    PT2Dict.to_csv('CASPT2.csv')        \n",
    "    return PT2Dict.set_index('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CASPT2=caspt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to grab the whole row\n",
    "# caspt2().loc[caspt2()['Labels'] =='ethylene_7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab casscf energies \n",
    "def casscf():\n",
    "    SCF=[]\n",
    "    SCFList=[]\n",
    "    SCFlabels=[]\n",
    "    for j in structs:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),f'{j}_dir'))==True:\n",
    "            path=os.path.join(os.getcwd(),f'{j}_dir',f'PT2_ozone_{j}.output')\n",
    "            SCFlabels.append(j)\n",
    "            SCF.append(float((grep['-i', '::    RASSCF root number  1',path] | awk['{print $8 }'])()))\n",
    "    SCFList=np.array(SCF)\n",
    "    SCFDict=pd.DataFrame({'Label':SCFlabels,'SCF':SCFList})\n",
    "    SCFDict.to_csv('CASSCF.csv')    \n",
    "    return SCFDict.set_index('Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CASSCF=casscf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(df_CASSCF-df_CASSCF.max())\n",
    "plt.plot(df_CASPT2-df_CASPT2.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab PT2 energies \n",
    "def PT2():\n",
    "    E2=[]\n",
    "    E2List=[]\n",
    "    E2labels=[]\n",
    "    ES_E2=[]\n",
    "    ES_E2List=[]\n",
    "    ES_E2labels=[]\n",
    "\n",
    "    for j in structs:\n",
    "        if os.path.exists(os.path.join(os.getcwd(),f'{j}_dir'))==True:\n",
    "            path=os.path.join(os.getcwd(),f'{j}_dir',f'PT2_ozone_{j}.output')\n",
    "            E2labels.append(j)\n",
    "            E2.append(float((grep['-i', 'E2 (Variational):',path] | awk['{print $NF }'])()))\n",
    "    E2List=np.array(E2)\n",
    "    E2Dict=pd.DataFrame({'Label':E2labels,'E2':E2List})\n",
    "    df=E2Dict.set_index('Label')\n",
    "    df.to_csv('E2.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E2Dict=PT2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(E2Dict-E2Dict.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab file paths for the directories\n",
    "# onlyfiles = [f for f in os.listdir(os.getcwd()) if os.path.isdir(os.path.join(os.getcwd(), f))]\n",
    "onlyfiles = (f for f in os.listdir(os.getcwd()) if os.path.isdir(os.path.join(os.getcwd(), f)))\n",
    "for entry in os.scandir(path=os.getcwd()):\n",
    "    if entry.is_dir():\n",
    "        if entry.name=='Fock':\n",
    "            path_to_Focks=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='e2':\n",
    "            path_to_e2=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='Labels':\n",
    "            path_to_Labels=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='IVECW':\n",
    "            path_to_IVECW=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='1_ints':\n",
    "            path_to_1_ints=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='2_ints':\n",
    "            path_to_2_ints=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='1_int_idx':\n",
    "            path_to_1_int_idx=os.path.join(os.getcwd(),entry.name)\n",
    "        if entry.name=='2_int_idx':\n",
    "            path_to_2_int_idx=os.path.join(os.getcwd(),entry.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typ_exists=sorted(sum(list([j.replace('GMJ_e2_','') for j in i.split('/')[-1].split('.') if 'GMJ' in j] for i in glob.glob(os.path.join(path_to_e2,f'{structs[0]}.GMJ_e2_*'))),[]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exists.pickle', 'wb') as handle:\n",
    "    pickle.dump(typ_exists, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "#   Keep everything at float64\n",
    "DTYPE = np.float_\n",
    "# DTYPE = np.float16\n",
    "\n",
    "#   Create an array with the easy data\n",
    "def createArrray(filename):\n",
    "    files = sorted(glob.glob(filename))\n",
    "    arrayname = []\n",
    "    for i in sorted(files):\n",
    "        arrayname.append(\n",
    "            np.stack(\n",
    "                np.array(pd.read_csv(i, header=None),\n",
    "                         dtype=DTYPE,\n",
    "                         copy=False).flatten()))\n",
    "\n",
    "    arrayname = np.asarray(arrayname, dtype=DTYPE)\n",
    "    return arrayname\n",
    "#   Start transforming the HDF5 files from the data directory\n",
    "h5list = []\n",
    "for i in structs:\n",
    "    h5list.append(os.path.join(os.getcwd(),'hdf5',f'PT2_ozone_{str(i)}.rasscf.h5'))\n",
    "# Grab AO_OVERLAP_MATRIX,MO_ENERGIES,MO_OCCUPATIONS,MO_VECTORS,MO_TYPEINDICES,NACTEL,NBAS from hdf5\n",
    "def h5feats(h5list):\n",
    "    f = h5.File(h5list[0], 'r')\n",
    "    datasetNames = [n for n in f.keys()]\n",
    "    b = []\n",
    "    labels = []\n",
    "    # AO_FOCKINT_MATRIX=[]\n",
    "    # Useful attributes from the hdf5 files\n",
    "    NBAS=[]\n",
    "    NACTEL=[]\n",
    "    for k, elem in enumerate(structs):\n",
    "        for count, ele in enumerate([i for i in f.attrs]):\n",
    "            if ele =='NBAS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r').attrs[ele]).reshape(-1)):\n",
    "                    NBAS.append(elemt)\n",
    "            if ele =='NACTEL':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r').attrs[ele]).reshape(-1)):\n",
    "                    NACTEL.append(elemt)\n",
    "\n",
    "\n",
    "    MO_ENERGIES=[]\n",
    "    MO_OCCUPATIONS=[]\n",
    "    MO_TYPEINDICES=[]\n",
    "    MO_VECTORS=[]\n",
    "    t0=time()\n",
    "    #   Eliminate certain features that won't be good for regression\n",
    "    for k, elem in enumerate(structs):\n",
    "        for count, ele in enumerate([n for n in h5.File(h5list[k], 'r').keys()]):\n",
    "            if ele =='MO_TYPEINDICES':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_TYPEINDICES.append(elemt)\n",
    "\n",
    "            if ele =='MO_ENERGIES':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_ENERGIES.append(elemt)\n",
    "\n",
    "            if ele =='MO_OCCUPATIONS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_OCCUPATIONS.append(elemt)\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "    print(f'time: {time()-t0} s')\n",
    "    # AO_FOCKINT_MATRIX=np.array(AO_FOCKINT_MATRIX).reshape(len(dislist),int(NBAS[0]),int(NBAS[0]))\n",
    "    MO_ENERGIES= np.array(MO_ENERGIES).reshape(len(structs),int(NBAS[0]))\n",
    "    MO_OCCUPATIONS= np.array(MO_OCCUPATIONS).reshape(len(structs),int(NBAS[0]))\n",
    "    MO_TYPEINDICES=np.array(MO_TYPEINDICES).reshape(len(structs),int(NBAS[0]))\n",
    "    \n",
    "    return MO_OCCUPATIONS,MO_TYPEINDICES,NACTEL,NBAS\n",
    "\n",
    "\n",
    "MO_OCCUPATIONS,MO_TYPEINDICES,NACTEL,NBAS=h5feats(h5list)\n",
    "\n",
    "\n",
    "h5list_scf = []\n",
    "for i in structs:\n",
    "    h5list_scf.append(os.path.join(os.getcwd(),'hdf5',f'PT2_ozone_{str(i)}.scf.h5'))\n",
    "# Grab AO_OVERLAP_MATRIX,MO_ENERGIES,MO_OCCUPATIONS,MO_VECTORS,MO_TYPEINDICES,NACTEL,NBAS from hdf5\n",
    "def MO_VEC(h5list_scf):\n",
    "    f = h5.File(h5list_scf[0], 'r')\n",
    "    datasetNames = [n for n in f.keys()]\n",
    "    b = []\n",
    "    labels = []\n",
    "    # AO_FOCKINT_MATRIX=[]\n",
    "    # Useful attributes from the hdf5 files\n",
    "    NBAS=[]\n",
    "    NACTEL=[]\n",
    "    for k, elem in enumerate(structs):\n",
    "        for count, ele in enumerate([i for i in f.attrs]):\n",
    "            if ele =='NBAS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r').attrs[ele]).reshape(-1)):\n",
    "                    NBAS.append(elemt)\n",
    "    MO_VECTORS=[]\n",
    "    MO_ENERGIES=[]  \n",
    "    MO_OCCUPATIONS=[]\n",
    "    t0=time()\n",
    "    #   Eliminate certain features that won't be good for regression\n",
    "    for k, elem in enumerate(structs):\n",
    "        for count, ele in enumerate([n for n in h5.File(h5list_scf[k], 'r').keys()]):\n",
    "            if ele =='MO_VECTORS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_VECTORS.append(elemt)\n",
    "            if ele =='MO_ENERGIES':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_ENERGIES.append(elemt)\n",
    "            if ele =='MO_OCCUPATIONS':\n",
    "                for i, elemt in enumerate(np.array(h5.File(h5list_scf[k],'r')[ele]).reshape(-1)):\n",
    "                    MO_OCCUPATIONS.append(elemt)\n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "\n",
    "    print(f'time: {time()-t0} s')\n",
    "    MO_ENERGIES= np.array(MO_ENERGIES).reshape(len(structs),int(NBAS[0]))\n",
    "    MO_OCCUPATIONS= np.array(MO_OCCUPATIONS).reshape(len(structs),int(NBAS[0]))\n",
    "    MO_VECTORS=np.array(MO_VECTORS).reshape(len(structs),int(NBAS[0]),int(NBAS[0]))\n",
    "    \n",
    "    return MO_VECTORS,MO_ENERGIES,MO_OCCUPATIONS\n",
    "\n",
    "\n",
    "MO_VECTORS,scf_F,scf_OCC=MO_VEC(h5list_scf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_indx(list_of_dicts):\n",
    "    indx=[]\n",
    "    for i in list_of_dicts.keys():\n",
    "        if len(list_of_dicts[i])>0:\n",
    "            indx.append(list(list_of_dicts[i].keys()))\n",
    "    return indx[0]\n",
    "\n",
    "\n",
    "path_check=os.path.join(os.getcwd(),f'{structs[0]}_dir',f'PT2_ozone_{structs[0]}.output')\n",
    "\n",
    "# Sanity check...\n",
    "# REMEVDZPER FROZEN CORE APPROXIMATION\n",
    "# Number of frozen orbitals\n",
    "fro=int(subprocess.Popen(f\"grep -i 'Frozen orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "# Number of inactive orbitals\n",
    "inact=int(subprocess.Popen(f\"grep -i 'Inactive orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "# Number of active orbitals\n",
    "act=int(subprocess.Popen(f\"grep -i 'Active orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "# Number of seconary orbitals\n",
    "virt=int(subprocess.Popen(f\"grep -i 'Secondary orbitals' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "# Number of basis functions for sanity check\n",
    "bas_check=int(subprocess.Popen(f\"grep -i 'Number of basis functions' {path_check} | tail -n 1\",shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT).communicate()[0].split()[-1])\n",
    "\n",
    "Basis_Indices=[]\n",
    "for i in range(fro):\n",
    "    Basis_Indices.append(f'F{i+1}')\n",
    "for i in range(inact):\n",
    "    Basis_Indices.append(f'I{i+1}')\n",
    "for i in range(act):\n",
    "    Basis_Indices.append(f'A{i+1}')\n",
    "for i in range(virt):\n",
    "    Basis_Indices.append(f'S{i+1}')    \n",
    "    \n",
    "print(f'Basis sanity check passed={bas_check==len(Basis_Indices)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the labels that match the IVECW and IVECC2 files\n",
    "def gen_labels(typ):\n",
    "    Labels=[]\n",
    "    Indexes=[]\n",
    "    i=structs[0]\n",
    "    return [j.split()[0].replace('\\n','').replace('00','').replace('S0','S').replace('I0','I').replace(',','') for j in open(os.path.join(path_to_Labels,f'{i}.GMJ_RHS_{typ}.csv'),'r').readlines()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair_labels(typ):\n",
    "    Labels=[]\n",
    "    Indexes=[]\n",
    "    i=structs[0]\n",
    "    return sorted(set(['_'.join(j.split()[0].replace('\\n','').replace('00','').replace('S0','S').replace('I0','I').replace(',','').split('_')[0:2]) for j in open(os.path.join(path_to_Labels,f'{i}.GMJ_RHS_{typ}.csv'),'r').readlines()]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(typ,len(gen_labels(typ)),len(gen_pair_labels(typ))) for typ in typ_exists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dim_dict(typ_exists):\n",
    "    '''    \n",
    "    Dimension check for DDCASPT2: check the ordering of the pair-energies,\n",
    "    this notation follows a mix of the papers and code.\n",
    "    \n",
    "    A (IA->AA): \\n TIUV \\n E_{ti} E_{uv} \\n pqrs=tiuv=0123 \\n    \n",
    "    B_P (II->AA) (P): \\n IJTU \\n E_{ti} E_{uj} \\n pqrs=tiuj=2031 \\n\n",
    "    B_M (II->AA) (M): \\n IJTU \\n E_{ti} E_{uj} \\n pqrs=tiuj=2031 \\n\n",
    "    C (AA->VA): \\n UVAT \\n E_{at} E_{uv} \\n pqrs=atuv=2301 \\n\n",
    "    D (IA->VA/AV): \\n IUAT/IUTA \\n E_{ai} E_{tu}/E_{ti} E_{au} \\n pqrs=(a/t)i(t/a)u=2031 \\n\n",
    "    E_P (II->AV) (P): \\n IJAT \\n E_{ti} E_{aj} \\n pqrs=tiaj=3021 \\n\n",
    "    E_M (II->AV) (M): \\n IJAT \\n E_{ti} E_{aj} \\n pqrs=tiaj=3021 \\n\n",
    "    F_P (AA->VV) (P): \\n TUAB \\n E_{at} E_{bu} \\n pqrs=atbu=2031 \\n\n",
    "    F_M (AA->VV) (M): \\n TUAB \\n E_{at} E_{bu} \\n pqrs=atbu=2031 \\n\n",
    "    G_P (IA->VV) (P): \\n ITAB \\n E_{ai} E_{bt} \\n pqrs=aibt=2031 \\n\n",
    "    G_M (IA->VV) (M): \\n ITAB \\n E_{ai} E_{bt} \\n pqrs=aibt=2031 \\n\n",
    "    H_P (II->VV) (P): \\n IJAB \\n E_{ai} E_{bj} \\n pqrs=aibj=2031 \\n\n",
    "    H_M (II->VV) (M): \\n IJAB \\n E_{ai} E_{bj} \\n pqrs=aibj=2031 \\n\n",
    "    '''    \n",
    "    i=structs[0]\n",
    "    dims=[]\n",
    "    for typ in typ_exists:\n",
    "        dims.append((typ,np.array([i.split('=')[-1].split('x') for i in open(os.path.join(path_to_e2,f'{i}.GMJ_e2_{typ}.csv'),'r').readlines() if 'mat. size =' in i ]).flatten().astype(int)))\n",
    "    return dict(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_dict=gen_dim_dict(typ_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip(lst):   \n",
    "    return '_'.join(i.replace('A00','A').replace('I00','I').replace('S00','S').replace('I0','I').replace('A0','A').replace('S0','S') for i in lst.split('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ordered(typ):\n",
    "    '''\n",
    "    Return a dataframe for each type\n",
    "    Index=proper indexing\n",
    "    level_0=row\n",
    "    level_1=column\n",
    "    0=W value\n",
    "    '''\n",
    "    i=structs[0]\n",
    "    ordered=pd.read_csv(os.path.join(path_to_IVECW,f'{i}.GMJ_IVECW_{typ}.csv'),delim_whitespace=True, skiprows=[0],header=None).astype(np.float64).dropna(axis=1)\n",
    "    ordered.columns=list(range(len(ordered.columns)))\n",
    "    ordered=ordered.stack()\n",
    "    df=pd.read_csv(os.path.join(path_to_Labels,f'{i}.GMJ_RHS_{typ}.csv'),header=None,delimiter=',',index_col=0)\n",
    "    df.index=list(map(strip,df.index))\n",
    "    merged=ordered.reset_index().sort_values(by=0).set_index(df.sort_values(by=1).index).sort_values(['level_0','level_1'])    \n",
    "\n",
    "#     print(f'Type {typ} all are correct={(merged[0].values.flatten()==ordered.values.flatten()).all()==True}')\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate IVECW\n",
    "def gen_e2(typ):\n",
    "    e2=[]\n",
    "    proper_labels=gen_labels(typ)\n",
    "    for i in structs:\n",
    "        df=pd.read_csv(os.path.join(path_to_e2,f'{i}.GMJ_e2_{typ}.csv'),delim_whitespace=True, skiprows=[0],header=None).astype(np.float64).dropna(axis=1).stack()\n",
    "        df.index=gen_ordered(typ).index\n",
    "        df=df.to_frame(name=str(i))\n",
    "        e2.append(df)\n",
    "    df1=pd.concat(e2,axis=1).loc[proper_labels]\n",
    "    df1.index=[i for idx,i in enumerate(proper_labels)]\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_pair(typ):\n",
    "    Y=gen_e2(typ).astype(float)\n",
    "# Needs to be qs, we're summing over the occupied orbitals    \n",
    "    Y_pair_set=list(set(['_'.join((i.split('_')[0],i.split('_')[1]))+'_' for i in Y.index.tolist()]))\n",
    "    Y_pair_df=pd.concat([Y[Y.index.str.find(j)==0].sum() for j in Y_pair_set],axis=1)\n",
    "    Y_pair_df.columns=list(set(['_'.join((i.split('_')[0],i.split('_')[1])) for i in Y.index.tolist()]))\n",
    "    return Y_pair_df.T.sort_index().groupby(level=0).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_label(typ):\n",
    "    if f'{typ}_M' in typ_exists and f'{typ}_P' in typ_exists:\n",
    "        return gen_pair_labels(f'{typ}_P')+gen_pair_labels(f'{typ}_M')\n",
    "    elif f'{typ}_P' in typ_exists:\n",
    "        return gen_pair_labels(f'{typ}_P')\n",
    "    elif f'{typ}_M' in typ_exists:\n",
    "        return gen_pair_labels(f'{typ}_M')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_e2(typ):\n",
    "    if f'{typ}_M' in typ_exists and f'{typ}_P' in typ_exists:\n",
    "        df=pd.concat([gen_pair(f'{typ}_M'),gen_pair(f'{typ}_P')],axis=0).groupby(level=0).sum()\n",
    "#         df.index=[i for idx,i in enumerate(stack_label(typ))]\n",
    "        return df\n",
    "    elif f'{typ}_P' in typ_exists:\n",
    "        return gen_pair(f'{typ}_P').groupby(level=0).sum()\n",
    "    elif f'{typ}_M' in typ_exists:\n",
    "        return gen_pair(f'{typ}_M').groupby(level=0).sum()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "for typ in set([i.split('_')[0] for i in typ_exists ]):\n",
    "    if typ=='A':\n",
    "        typA_e2=gen_pair(f'{typ}')        \n",
    "        typA_labels=gen_pair_labels(typ)\n",
    "    if typ=='B':        \n",
    "        typB_e2=stack_e2(typ)\n",
    "        typB_labels=stack_label(typ)        \n",
    "    if typ=='C':\n",
    "        typC_e2=gen_pair(f'{typ}')\n",
    "        typC_labels=gen_pair_labels(f'{typ}')\n",
    "    if typ=='D':        \n",
    "        typD_labels=gen_pair_labels(f'{typ}')\n",
    "        typD_e2=gen_pair(f'{typ}')\n",
    "    if typ=='E':\n",
    "        typE_e2=stack_e2(typ)\n",
    "        typE_labels=stack_label(typ)\n",
    "    if typ=='F':        \n",
    "        typF_e2=stack_e2(typ)\n",
    "        typF_labels=stack_label(typ)\n",
    "    if typ=='G':        \n",
    "        typG_e2=stack_e2(typ)\n",
    "        typG_labels=stack_label(typ)\n",
    "    if typ=='H':  \n",
    "        typH_e2=stack_e2(typ)\n",
    "        typH_labels=stack_label(typ)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(typA_labels+typB_labels+typC_labels+typD_labels+typE_labels+typF_labels+typG_labels+typH_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_e2=pd.concat([gen_e2(typ) for typ in typ_exists]).groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E2Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(E2Dict,stacked_e2.sum())\n",
    "plt.plot(E2Dict.values,E2Dict.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# big_40=stacked_e2.T.abs().describe().loc['mean'].sort_values(ascending=False).nlargest(500).index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pairs=pd.concat([typA_e2,typB_e2,typC_e2,typD_e2,typE_e2,typF_e2,typG_e2,typH_e2]).groupby(level=0).sum()\n",
    "pair_labels=stacked_pairs.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typ_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_stack=pd.concat([gen_e2(typ) for typ in typ_exists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(E2Dict,dummy_stack.sum())\n",
    "plt.scatter(dummy_stack.sum(),stacked_e2.sum())\n",
    "plt.plot(E2Dict.values,E2Dict.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab molecular orbital occupations and make it into a dataframe labeled with xyz file name\n",
    "MO_OCC=[]\n",
    "for j in range(len(structs)):\n",
    "    MO_OCC.append(dict(zip(Basis_Indices,[i for i in list(MO_OCCUPATIONS[j])])))\n",
    "MO_OCC_Dict=dict(zip([str(k) for k in structs],MO_OCC))\n",
    "MO_OCC_DF=pd.DataFrame(MO_OCC_Dict)\n",
    "\n",
    "# Dataframe of MO occupation, index=basis indices and columns=structs\n",
    "MO_OCCUPATIONS_DF=pd.DataFrame(MO_OCCUPATIONS,index=structs,columns=Basis_Indices).transpose()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Keep in mind HDF5 zeroes out the actrive orbitals... we'll use the Fock matrix to recover these\n",
    "# \n",
    "# Grab molecular orbital energy and make it into a dataframe labeled with xyz file name\n",
    "MO_ENERGIES=[]\n",
    "for j in structs:\n",
    "    MO_ENERGIES.append(np.genfromtxt(os.path.join(path_to_Focks,f'{j}.GMJ_Fock_MO.csv'), delimiter=''))\n",
    "\n",
    "\n",
    "# Dataframe of MO energies, index=basis indices and columns=structs\n",
    "MO_ENERGIES_DF=pd.DataFrame(MO_ENERGIES,index=structs,columns=Basis_Indices).transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Basis_Indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_one_int():\n",
    "    one_int=[]\n",
    "    Labels=[]\n",
    "    Indexes=[]\n",
    "    upd_1int_indx=[]\n",
    "    def one_gener(i):\n",
    "        return pd.DataFrame(np.genfromtxt(os.path.join(path_to_1_ints,f'{i}.GMJ_one_int.csv'), delimiter='',dtype=float),index=Basis_Indices,columns=Basis_Indices)\n",
    "            \n",
    "\n",
    "#     Dict=dict(zip(Indexes,Labels))\n",
    "    return dict((i,one_gener(i)) for i in structs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time()\n",
    "int1=gen_one_int()\n",
    "print(f'Integrals loaded in {time()-t0:0.4f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option(\"precision\", 2)\n",
    "# np.set_printoptions(precision=2)\n",
    "# pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "\n",
    "\n",
    "nmo=len(Basis_Indices)\n",
    "indice=[]\n",
    "ad_ind=[]\n",
    "for ind,i in enumerate(range(nmo)):\n",
    "    for indx,j in enumerate(range(nmo)):\n",
    "        ad_ind.append(f'{i+1}_{j+1}')\n",
    "        if j<=i:\n",
    "            indice.append(f'{i+1}_{j+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(indice),len(ad_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raw_MO=pd.DataFrame(np.genfromtxt(os.path.join(path_to_2_ints,f'{structs[0]}.GMJ_two_int.csv'), delimiter='',dtype=float),index=indice,columns=indice)\n",
    "\n",
    "# # This is wrong\n",
    "# for ind,a in enumerate(indice):\n",
    "#     for indx,b in enumerate(indice):\n",
    "#         if a!=b:\n",
    "#             print(a.split('_'),b.split('_'))\n",
    "#             # print(f'{j+1}_{i+1}',raw_MO[f'{j+1}_{i+1}'])\n",
    "#             # raw_MO.loc[f'{j+1}_{i+1}']=AO_DF.loc[f'{i+1}_{j+1}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def gen_MO(k):\n",
    "    if os.path.exists(os.path.join(path_to_2_ints,f'{k}.GMJ_two_int.csv'))==True:\n",
    "\n",
    "        raw_MO=np.genfromtxt(os.path.join(path_to_2_ints,f'{k}.GMJ_two_int.csv')).reshape(len(Basis_Indices),len(Basis_Indices),len(Basis_Indices),len(Basis_Indices))\n",
    "    return raw_MO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurelist=list()\n",
    "# def GenerateFeatures(wf_object, Miller=False, values=4):\n",
    "#     b =   wf_object.triplecheck\n",
    "\n",
    "#     #c = np.log10(np.absolute(wf_object.get_MO('oovv')*wf_object.t2start))\n",
    "#     ##infcheck=(c == -np.inf)\n",
    "#     #c[infcheck]=20\n",
    "\n",
    "#     #d = wf_object.pairs\n",
    "\n",
    "#     if Miller == False:\n",
    "#         for i in range(b.shape[0]):\n",
    "#             for j in range (b.shape[1]):\n",
    "#                 print('Top',i,j)\n",
    "#                 featurelist.clear()\n",
    "# # sort MP2 two-electron excitations e_{ij}^{ab} for ij\n",
    "# # (most negative, , smallest negative, , smallest positive, , largest positive)\n",
    "# # [:values]=first x values        \n",
    "# # [-values:]=last x values\n",
    "#                 ind=np.argsort(b[i,j].flatten(),axis=0)\n",
    "# # ij{MP2}\n",
    "#                 new=np.sum(b[i,j])#0\n",
    "# #                 featurelist.append('Pair_Energy')\n",
    "# # <ii||jj>        \n",
    "#                 new=np.hstack((new,wf_object.MO[i,i,j,j]))\n",
    "# # screen1 first 4 values (i) <ii||aa>\n",
    "#                 new=np.hstack((new,np.take_along_axis(wf_object.screen1[i,j].flatten(), ind, axis=0)[:values]))#1,2,3,4\n",
    "# # screen2 first 4 values (j) <jj||bb>\n",
    "#                 new=np.hstack((new,np.take_along_axis(wf_object.screen2[i,j].flatten(), ind, axis=0)[:values]))#9,10,11,12\n",
    "# # <aa||bb> matrix first 4 values \n",
    "#                 new=np.hstack((new,np.take_along_axis(wf_object.screenvirt[i,j].flatten(), ind, axis=0)[:values]))#9,10,11,12\n",
    "# # e_{ij}^{ab}{MP2} first 4 values\n",
    "#                 new=np.hstack((new,np.take_along_axis(b[i,j].flatten(), ind, axis=0)[:values]))#17,18,19,20\n",
    "#                 if i==j:       \n",
    "# # <ii||jj> is excluded since i==j                \n",
    "#                     ind=np.argsort(b[i,j].flatten(),axis=0)\n",
    "# # ij{MP2}                    \n",
    "#                     new=np.sum(b[i,j])#0\n",
    "# # screen1 first 4 values (i)\n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen1[i,j].flatten(), ind, axis=0)[:values]))#1,2,3,4\n",
    "# # screen2 first 4 values (j)       \n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen2[i,j].flatten(), ind, axis=0)[:values]))#9,10,11,12\n",
    "# # screen1 last 4 values (i)\n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen1[i,j].flatten(), ind, axis=0)[-values:]))#1,2,3,4\n",
    "# # screen2 last 4 values (j)\n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen2[i,j].flatten(), ind, axis=0)[-values:]))#9,10,11,12\n",
    "\n",
    "# # e_{ij}^{ab}{MP2} first 4 values                    \n",
    "#                     new=np.hstack((new,np.take_along_axis(b[i,j].flatten(), ind, axis=0)[:values]))#17,18,19,20\n",
    "# # e_{ij}^{ab}{MP2} last 4 values    \n",
    "#                     new=np.hstack((new,np.take_along_axis(b[i,j].flatten(), ind, axis=0)[-values:]))#17,18,19,20\n",
    "# # Missing ij{MP2} correlation in representation                    \n",
    "#                     one=np.sum(np.take_along_axis(b[i,j].flatten(), ind, axis=0)[-values:])\n",
    "#                     two=np.sum(np.take_along_axis(b[i,j].flatten(), ind, axis=0)[:values])\n",
    "#                     new=np.hstack((new, np.sum(b[i,j])-one-two))\n",
    "#                     featurelist.append('triplecheck1')\n",
    "#                 else:\n",
    "# # i!=j                    \n",
    "#                     ind=np.argsort(b[i,j].flatten(),axis=0)\n",
    "# # ij{MP2}    \n",
    "#                     new=np.sum(b[i,j])#0      \n",
    "# # <ii||jj>                    \n",
    "#                     new=np.hstack((new,wf_object.MO[i,i,j,j]))\n",
    "# # screen1 first 4 values (i)\n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen1[i,j].flatten(), ind, axis=0)[:values]))#1,2,3,4\n",
    "# # screen2 first 4 values (j)                    \n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen2[i,j].flatten(), ind, axis=0)[:values]))#9,10,11,12\n",
    "# # screen1 last 4 values (i)\n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen1[i,j].flatten(), ind, axis=0)[-values:]))#1,2,3,4\n",
    "# # screen2 last 4 values (j)                    \n",
    "#                     new=np.hstack((new,np.take_along_axis(wf_object.screen2[i,j].flatten(), ind, axis=0)[-values:]))#9,10,11,12\n",
    "# # e_{ij}^{ab}{MP2} first 4 values                                        \n",
    "#                     new=np.hstack((new,np.take_along_axis(b[i,j].flatten(), ind, axis=0)[:values]))#17,18,19,20\n",
    "# # e_{ij}^{ab}{MP2} last 4 values    \n",
    "#                     new=np.hstack((new,np.take_along_axis(b[i,j].flatten(), ind, axis=0)[-values:]))#17,18,19,20                    \n",
    "# # Missing ij{MP2} correlation in representation                    \n",
    "#                     one=np.sum(np.take_along_axis(b[i,j].flatten(), ind, axis=0)[-values:])\n",
    "#                     two=np.sum(np.take_along_axis(b[i,j].flatten(), ind, axis=0)[:values])\n",
    "#                     new=np.hstack((new, np.sum(b[i,j])-one-two))\n",
    "\n",
    "\n",
    "#                 if ((i==0) and (j==0)):\n",
    "#                     a=new.copy()\n",
    "#                     diag=wf_object.pairs[i,j]\n",
    "#                 elif ((i==0) and (j==1)):\n",
    "#                     g=new.copy()\n",
    "#                     offdiag=wf_object.pairs[i,j]\n",
    "#                 elif (i==j):        \n",
    "#                     a=np.vstack((a,new))#41\n",
    "#                     diag=np.vstack((diag,wf_object.pairs[i,j]))\n",
    "#                 elif (j > i):\n",
    "#                     g=np.vstack((g,new))#41\n",
    "#                     offdiag=np.vstack((offdiag,wf_object.pairs[i,j]))\n",
    "#         return a,diag,g,offdiag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time()\n",
    "gen_MO('110.0')\n",
    "print(time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "froz=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('F')]\n",
    "inact=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('I')]\n",
    "act=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('A')]\n",
    "virt=[indx for indx,i in enumerate(Basis_Indices) if i.startswith('S')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_F=dict(zip(structs,scf_F))\n",
    "# gen_occ=dict(zip(structs,scf_OCC))\n",
    "\n",
    "gen_F=MO_ENERGIES_DF\n",
    "gen_occ=MO_OCCUPATIONS_DF\n",
    "gen_F_SCF=pd.DataFrame(scf_F,columns=Basis_Indices,index=structs).T\n",
    "gen_occ_SCF=pd.DataFrame(scf_OCC,columns=Basis_Indices,index=structs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Basis_Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_set=sorted(set(sum([gen_pair_labels(typ) for typ in typ_exists],[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite the jacob style featurization step\n",
    "occcc=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=0])\n",
    "virttt=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen_big_4(object):\n",
    "    '''\n",
    "    Generate a set of i,j,k,l indices corresponding to the largest 4 two-electron excitations \n",
    "    per pair-energy.\n",
    "\n",
    "    !!!From here on out we need to use the same training set and test set.!!!\n",
    "    Indices:\n",
    "    i-internal\n",
    "    j-internal\n",
    "    k-external\n",
    "    l-external\n",
    "\n",
    "    '''    \n",
    "    # Pick 4 largest contributers to the pair energy\n",
    "    def big_4(self,dist):\n",
    "        return [['_'.join(k.split('_')[2:4]) for k in dummy_stack.loc[[ j for j in dummy_stack.index.tolist() if j.split('_')[0]==i.split('_')[0] and j.split('_')[1]==i.split('_')[1]]][dist].abs().sort_values(ascending=False).index[0:4].tolist()] for i in stacked_pairs.index.tolist()]\n",
    "    \n",
    "    def gen_ijkl(self):\n",
    "        # Dimensions: training index x pair energies x top 4 most frequent virtual orbitals for two electrion excitations\n",
    "        self.train_4=np.array([self.big_4(i) for i in train_ind])\n",
    "        # Most frequent: list of tuples containing (pair energy index, [top 4 most frequent virtual orbitals for two electrion excitations])\n",
    "        # This can serve as a label too.\n",
    "        self.train_freq=[(i,pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()) for idx,i in enumerate(stacked_pairs.index.tolist())]\n",
    "        # i,j, set of 4 largest [(k,l)]\n",
    "        self.set_ijkl_indices=[(Basis_Indices.index(i.split('_')[0]),Basis_Indices.index(i.split('_')[1]),[(Basis_Indices.index(k.split('_')[0]),Basis_Indices.index(k.split('_')[1])) for k in pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()]) for idx,i in enumerate(stacked_pairs.index.tolist())]\n",
    "        internal_basis=[i for i in Basis_Indices if 'S' not in i]\n",
    "        external_basis=[i for i in Basis_Indices if 'I' not in i]\n",
    "    # This is 1 indices per pair energy!        \n",
    "        set_i_indices=[]\n",
    "    # This is 1 indices per pair energy!            \n",
    "        set_j_indices=[]\n",
    "    # This is a set of 4 indices per pair energy!    \n",
    "        set_k_indices=[]\n",
    "    # This is a set of 4 indices per pair energy!    \n",
    "        set_l_indices=[]\n",
    "        for idx,i in enumerate(stacked_pairs.index.tolist()):\n",
    "            set_i_indices.append(internal_basis.index(i.split('_')[0]))\n",
    "            set_j_indices.append(internal_basis.index(i.split('_')[1]))\n",
    "            set_k_indices.append([external_basis.index(k.split('_')[0]) for k in pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()])\n",
    "            set_l_indices.append([external_basis.index(k.split('_')[1]) for k in pd.DataFrame(self.train_4[:,idx,:]).describe().loc['top'].tolist()])\n",
    "            \n",
    "        return set_i_indices,set_j_indices,set_k_indices,set_l_indices,self.train_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ijkl_idx=gen_big_4().gen_ijkl()\n",
    "set_i_indices=ijkl_idx[0]\n",
    "set_j_indices=ijkl_idx[1]\n",
    "set_k_indices=ijkl_idx[2]\n",
    "set_l_indices=ijkl_idx[3]\n",
    "set_indices=ijkl_idx[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "occcc=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=0])\n",
    "virttt=len(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=2])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slices for AA->AA indices\n",
    "# This will help zero out infinities\n",
    "internal_A=[idx for idx,i in enumerate(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=0].index) if 'A' in i]\n",
    "external_A=[idx for idx,i in enumerate(MO_OCC_DF.T.describe().loc['mean'][MO_OCC_DF.T.describe().loc['mean']!=2].index) if 'A' in i]\n",
    "\n",
    "inner_slice=slice(min(internal_A),max(internal_A)+1)\n",
    "outer_slice=slice(min(external_A),max(external_A)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class gen_two_ints(object):\n",
    "# Some of the D_{ij}^{ab}=f_{ii}+f_{jj}-f_{aa}-f_{bb}=e_{ii}+e_{jj}-e_{aa}-e_{bb} elements \n",
    "# will be 0 since ij and ab overlap for CASPT2\n",
    "# ij \\in {I,A}\n",
    "# ab \\in {A,V}\n",
    "# So ignore the warnings since they'll only be 0 when ijab \\in {A} only\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    def get_MO(self, string):\n",
    "        return self.MO[self.slice_dict[string[0]], self.slice_dict[string[1]],self.slice_dict[string[2]], self.slice_dict[string[3]]]\n",
    "\n",
    "    def get_F(self, string):\n",
    "        return self.F[self.slice_dict[string[0]], self.slice_dict[string[1]]] \n",
    "    \n",
    "    def compute_pairmatrix(self,selft2start,selfdoublecheck):\n",
    "        test = 2*selft2start*selfdoublecheck\n",
    "        test -= np.swapaxes(selft2start,2,3)*selft2start\n",
    "        c=np.sum(test,axis=(2,3))\n",
    "        return c    \n",
    "    \n",
    "    def build_tau(self,t2,t1):\n",
    "        ttau = t2.copy()\n",
    "        tmp = np.einsum('ia,jb->ijab', t1, t1,optimize=True)\n",
    "        ttau += tmp\n",
    "        return ttau    \n",
    "    \n",
    "#     def __init__(self,k):\n",
    "    def gen_feat(self,k):\n",
    "# Set variables\n",
    "# nocc:=Occupied orbitals\n",
    "# nvirt:=virtual orbitals\n",
    "# nfzc:=frozen orbitals\n",
    "# nmo:=number of orbitals\n",
    "        nocc=occcc\n",
    "        nvirt=virttt\n",
    "        self.nfzc=len(froz)\n",
    "        self.nocc=nocc\n",
    "        self.nvirt=nvirt\n",
    "        self.nmo=len(Basis_Indices)\n",
    "        nmo=nocc+nvirt\n",
    "# nmo=nocc+nvirt\n",
    "# Set slices\n",
    "# In CASPT2 occupied run I-A and virtual run A-S\n",
    "# Unlike MP2 there will be overlap between internal and external indices\n",
    "        self.slice_o = slice(0,nocc)\n",
    "        self.slice_v = slice(len(Basis_Indices)-virttt,len(Basis_Indices))\n",
    "        self.slice_a = slice(0,nmo)    \n",
    "\n",
    "# Dictionary with the slices        \n",
    "\n",
    "        self.slice_dict = {\n",
    "            'o': self.slice_o,\n",
    "            'v': self.slice_v,\n",
    "            'a': self.slice_a\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        featurelist=list()    \n",
    "# Virtual\n",
    "        self.empty=np.zeros((nvirt,))\n",
    "# Occupied    \n",
    "        self.occupado=np.zeros((nocc,))\n",
    "# MO integrals    \n",
    "        self.MO=gen_MO(k)\n",
    "# MO Fock matrix    \n",
    "        self.F=gen_F[k].to_numpy()\n",
    "# Zero out t1 matrix, dim(occ,virt)    \n",
    "        self.t1=np.zeros((nocc,nvirt))\n",
    "# MO Fock matrix internal\n",
    "        Focc = self.F[self.slice_o]\n",
    "# MO Fock matrix external    \n",
    "        Fvir = self.F[self.slice_v]  \n",
    "        self.orbocc=Focc\n",
    "        self.orbvirt=Fvir\n",
    "\n",
    "        self.Dia = Focc.reshape(-1, 1) - Fvir\n",
    "        self.Dijab = Focc.reshape(-1, 1, 1, 1) + Focc.reshape(-1, 1, 1) - Fvir.reshape(-1, 1) - Fvir \n",
    "\n",
    "        \n",
    "# Clean up AA block to get rid of infinities, we do not need this anyway              \n",
    "# t2=<ij||ab>/(e_i+e_j-e_a-e_b)\n",
    "# But this is <ij|ab>... idk...\n",
    "        self.t2start=self.MO[self.slice_o, self.slice_o, self.slice_v,self.slice_v] / self.Dijab\n",
    "# Zero out AA->AA\n",
    "        self.t2start[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "# Related to the correlation energy\n",
    "# np.einsum('ijab->',triplecheck)=MP2 correlation energy in jacob's code\n",
    "#dim(triplecheck)=(occ,occ,virt,virt)\n",
    "        self.triplecheck=2*self.t2start*self.get_MO('oovv')\n",
    "        self.triplecheck -=  np.swapaxes(self.get_MO('oovv'),2,3)*self.t2start \n",
    "# Zero out AA->AA\n",
    "        self.triplecheck[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "        \n",
    "        \n",
    "# Integral\n",
    "# doublecheck =<ij|ab>\n",
    "#dim(doublecheck)=(occ,occ,virt,virt)\n",
    "        self.doublecheck = self.MO[self.slice_o, self.slice_o, self.slice_v, self.slice_v]   \n",
    "# Zero out AA->AA\n",
    "        self.doublecheck[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "\n",
    "    \n",
    "# Not related to the correlation energy?\n",
    "# dim(pairenergy)=(occ,occ,virt,virt)\n",
    "# dim(compute_pairmatrix(t2start,doublecheck))=(occ,occ)->(occ,occ,virt,virt)\n",
    "        self.pairenergy=(np.zeros(self.doublecheck.shape)+self.compute_pairmatrix(self.t2start,self.doublecheck)[:,:,np.newaxis,np.newaxis])\n",
    "# Zero out AA->AA\n",
    "        self.pairenergy[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "\n",
    "\n",
    "# Related to the correlation energy\n",
    "#dim(pairs)=(virt,virt)\n",
    "# np.einsum('ab->',pairs)=np.einsum('ijab->',triplecheck)=MP2 correlation energy in jacob's code\n",
    "        tmp_tau = self.build_tau(self.t2start,self.t1)\n",
    "        self.pairs=2*tmp_tau*self.get_MO('oovv')\n",
    "        self.pairs-= np.swapaxes(self.get_MO('oovv'),2,3)*tmp_tau\n",
    "        self.pairs = np.sum(self.pairs,axis=(2,3))\n",
    "# Zero out AA->AA\n",
    "        self.pairs[inner_slice,outer_slice]=0        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        test=np.zeros(self.t2start.shape)\n",
    "        self.diag=test\n",
    "        for i in range (0,self.nocc):\n",
    "            for j in range (0,self.nocc):\n",
    "                np.fill_diagonal(self.diag[i,j,:,:],1)\n",
    "\n",
    "# Dim(temp)=(occ,virt)\n",
    "# Basically dim(<ii|jj>)->dim(<i|j>)\n",
    "        temp=np.zeros((self.nocc,self.nvirt))\n",
    "        for i in range (0,self.nocc):\n",
    "            for j in range (0,self.nvirt):\n",
    "                temp[i,j]=self.doublecheck[i,i,j,j]\n",
    "# Dim(test1)=(occ,occ,virt,virt)                \n",
    "        test1=np.zeros((self.t2start.shape))\n",
    "\n",
    "# dim(temp)=(occ,virt) -> dim(temp[:,np.newaxis,:,np.newaxis])=(occ,1,virt,1)\n",
    "# np.newaxis makes this a dummy index, : will be unique\n",
    "# i.e. (occ,copy,virt,copy)\n",
    "# (occ,occ,virt,virt) + (occ,1,virt,1)\n",
    "# <ii||aa>    \n",
    "        self.screen1=test1+temp[:,np.newaxis,:,np.newaxis]\n",
    "# Zero out AA->AA\n",
    "        self.screen1[inner_slice,inner_slice,outer_slice,outer_slice]=0            \n",
    "\n",
    "# dim(temp)=(occ,virt) -> dim(temp[np.newaxis,:,np.newaxis,:])=(1,occ,1,virt)\n",
    "# (occ,occ,virt,virt) + (1,occ,1,virt)\n",
    "# np.newaxis makes this a dummy index, : will be unique\n",
    "# i.e. (copy,occ,copy,virt)\n",
    "# <jj||bb>    \n",
    "        self.screen2=test1+temp[np.newaxis,:,np.newaxis,:]\n",
    "# Zero out AA->AA\n",
    "        self.screen2[inner_slice,inner_slice,outer_slice,outer_slice]=0                \n",
    "# screen1[i,j,k,l]==screen2[j,i,l,k].T\n",
    "\n",
    "# nfzc=# frozen core\n",
    "        val=self.nmo-self.nfzc\n",
    "# Dim(temp)=(nmo,nmo)\n",
    "# Basically dim(<ii|jj>)->dim(<i|j>)    \n",
    "        temp=np.zeros((val,val))        \n",
    "        for i in range (0,val):\n",
    "            for j in range (0,val):\n",
    "                temp[i,j]=self.MO[i,i,j,j]\n",
    "# Get virtual indices: <aa|bb> basically\n",
    "# <aa||bb>\n",
    "# (occ,occ,virt,virt) + (virt,virt)\n",
    "# (occ,occ,virt,virt) + (1,1,virt,virt)                \n",
    "        temp =temp[self.slice_v,self.slice_v]\n",
    "        self.screenvirt=test1+temp[np.newaxis,np.newaxis,:,:]\n",
    "# Zero out AA->AA\n",
    "        self.screenvirt[inner_slice,inner_slice,outer_slice,outer_slice]=0\n",
    "        # b=(i,j,a,b)=(13, 13, 5, 5)\n",
    "        b=self.triplecheck\n",
    "# Zero out AA->AA\n",
    "        diag_indx=[]\n",
    "        off_diag_indx=[]     \n",
    "        featurelist=[]\n",
    "        featurelist.clear()\n",
    "        feature=[]\n",
    "        feature.clear()    \n",
    "        index=['pair_energy','coulomb','screen1_1','screen1_2','screen1_3','screen1_4','screen2_1','screen2_2','screen2_3','screen2_4','eijab_1','eijab_2','eijab_3','eijab_4','screenvirt_1','screenvirt_2','screenvirt_3','screenvirt_4']        \n",
    "        for idx,i in enumerate([j for j,v in set_indices]):\n",
    "# ij{MP2}              \n",
    "            new=np.sum(b[set_i_indices[idx],set_j_indices[idx]])#0\n",
    "# <ii||jj>    \n",
    "            new=np.hstack((new,self.MO[set_i_indices[idx],set_i_indices[idx],set_j_indices[idx],set_j_indices[idx]]))\n",
    "# <ii||aa>    \n",
    "            new=np.hstack((new,np.array([self.screen1[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "# <jj||bb>    \n",
    "            new=np.hstack((new,np.array([self.screen2[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "# e_{ij}^{ab} MP2\n",
    "            new=np.hstack((new,np.array([b[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "# # <aa||bb>    \n",
    "            new=np.hstack((new,np.array([self.screenvirt[set_i_indices[idx],set_j_indices[idx],k,l] for k,l in zip(set_k_indices[idx],set_l_indices[idx])]).flatten()))\n",
    "            featurelist.append((i,new))\n",
    "        return pd.DataFrame(dict(featurelist),index=index).T\n",
    "#         return pd.concat([pd.DataFrame(self.a,index=diag_indx),pd.DataFrame(self.g,index=off_diag_indx)]).loc[full_set]\n",
    "\n",
    "# gen_feat\n",
    "#     self.dict=dict([(k,gen_feat(k))for k in structs])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elements(x):\n",
    "    '''\n",
    "    Takes an integer, x, and returns the number of off-diagonal elements of an upper triangular matrix\n",
    "    f(x)=(x*(x-1))/2\n",
    "    '''\n",
    "    return (x*(x-1))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_indices[0][0],set_indices[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_1_feats(k):\n",
    "    # t0=time()\n",
    "    # For an e_pq, there should be 10 F, 10 occs, and then 10 SCF F and 10 SCF occs\n",
    "    Fp=[]\n",
    "    Fq=[]\n",
    "\n",
    "    occp=[]\n",
    "    occq=[]\n",
    "\n",
    "    SCFFp=[]\n",
    "    SCFFq=[]\n",
    "\n",
    "\n",
    "    SCFOCCp=[]\n",
    "    SCFOCCq=[]\n",
    "\n",
    "    hpp=[]\n",
    "    hqq=[]\n",
    "    hrr=[]\n",
    "    hss=[]\n",
    "\n",
    "    rs_df=[]\n",
    "    h=int1[k]\n",
    "    for idx,i in enumerate(set_indices):\n",
    "\n",
    "        p,q=i[0].split('_')\n",
    "        # print(p,q)\n",
    "        Fp.append(gen_F.loc[p,k])\n",
    "        Fq.append(gen_F.loc[q,k])\n",
    "        occp.append(gen_occ.loc[p,k])\n",
    "        occq.append(gen_occ.loc[q,k])\n",
    "\n",
    "        SCFFp.append(gen_F_SCF.loc[p,k])\n",
    "        SCFFq.append(gen_F_SCF.loc[q,k])\n",
    "        SCFOCCp.append(gen_occ_SCF.loc[p,k])\n",
    "        SCFOCCq.append(gen_occ_SCF.loc[q,k])\n",
    "        hpp.append(h.loc[p,p])\n",
    "        hqq.append(h.loc[p,p])\n",
    "\n",
    "\n",
    "\n",
    "        Fr=[]\n",
    "        Fs=[]\n",
    "        occr=[]\n",
    "        occs=[]\n",
    "        SCFFr=[]\n",
    "        SCFFs=[]\n",
    "        SCFOCCr=[]\n",
    "        SCFOCCs=[]\n",
    "        for idxx,j in enumerate(i[1]):\n",
    "            r,s=j.split('_')\n",
    "\n",
    "            Fr.append((f'Fr{idxx+1}',gen_F.loc[r,k]))\n",
    "            Fs.append((f'Fs{idxx+1}',gen_F.loc[s,k]))\n",
    "\n",
    "            occr.append((f'occr{idxx+1}',gen_occ.loc[r,k]))\n",
    "            occs.append((f'occs{idxx+1}',gen_occ.loc[s,k]))\n",
    "\n",
    "            SCFFr.append((f'SCFFr{idxx+1}',gen_F_SCF.loc[r,k]))\n",
    "            SCFFs.append((f'SCFFs{idxx+1}',gen_F_SCF.loc[s,k]))\n",
    "\n",
    "\n",
    "            SCFOCCr.append((f'SCFOCCr{idxx+1}',gen_occ_SCF.loc[r,k]))\n",
    "            SCFOCCs.append((f'SCFOCCs{idxx+1}',gen_occ_SCF.loc[s,k]))\n",
    "\n",
    "            hrr.append((f'hrr{idxx+1}',h.loc[r,r]))\n",
    "            hss.append((f'hss{idxx+1}',h.loc[s,s]))\n",
    "\n",
    "\n",
    "        rs_df.append(pd.DataFrame.from_dict({**dict(Fr),**dict(Fs),**dict(occr),**dict(occs),**dict(SCFFr),**dict(SCFFs),**dict(SCFOCCr),**dict(SCFOCCs),**dict(hrr),**dict(hss)},orient='index',columns=[i[0]]))\n",
    "    rs=pd.concat(rs_df,axis=1).T\n",
    "    dummy_df=pd.DataFrame({'hpp':hpp,'hqq':hqq,'Fp':Fp,'Fq':Fq,'occp':occp,'occq':occq,'SCFFp':SCFFp,'SCFFq':SCFFq,'SCFOCCp':SCFOCCp,'SCFOCCq':SCFOCCq},index=pair_labels)\n",
    "    # print(f'{k} {time()-t0}')\n",
    "    return pd.concat([rs,dummy_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_two_el():\n",
    "    return dict([(k,gen_two_ints().gen_feat(k))for k in structs])\n",
    "\n",
    "\n",
    "def gen_one_diag():\n",
    "    return dict([(k,gen_1_feats(k)) for k in structs])\n",
    "\n",
    "\n",
    "def gen_bin():\n",
    "    # FSO=From same orbital\n",
    "    FSO=[]\n",
    "    # Indexing\n",
    "    featind=[]\n",
    "    # Keys=Structs in string format\n",
    "    keys=[]\n",
    "    for i in structs:\n",
    "        k=str(i)\n",
    "        keys.append(k)\n",
    "        for ind,g in enumerate(full_set):\n",
    "# Epq Ers\n",
    "# # e_q+e_s-e_p-e_r\n",
    "# TIUV\n",
    "#'A'+g[0]+'_I'+g[1]+''+g[2]+''+g[3]\n",
    "# Eti Euv (E01 E23)\n",
    "# e_i+e_v-e_u-e_t\n",
    "# e[0] + e[3] - e[1] - e[2]\n",
    "            featind.append(g)\n",
    "            idx=g.split('_')\n",
    "            q=idx[0]\n",
    "            s=idx[1]\n",
    "# Since I!=A just append 0 since they'll never both come from the same orbital                \n",
    "            if q==s:\n",
    "                FSO.append(1)\n",
    "            else:\n",
    "                FSO.append(0)\n",
    "    return dict([(z,pd.DataFrame({'From_Same_Orbital':np.array(FSO).reshape(len(structs),-1)[idx]},index=np.array(featind).reshape(len(structs),-1)[idx])) for idx,z in enumerate(keys)])               \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Big_Data_GS():\n",
    "    t0=time()\n",
    "    with open('fixed_feats.pickle', 'wb') as handle:\n",
    "        pickle.dump(pd.concat([pd.concat({k: v for k,v in gen_bin().items()},axis=0),\n",
    "                               pd.concat({k: v for k,v in gen_two_el().items()},axis=0),\n",
    "                               pd.concat({k: v for k,v in gen_one_diag().items()},axis=0)],axis=1), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('fixed_targets.pickle', 'wb') as handle:\n",
    "        pickle.dump(stacked_pairs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Big_Data_GS()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle(f'fixed_feats.pickle'),\n",
    "# pd.read_pickle(f'typH_targets.pickle').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
